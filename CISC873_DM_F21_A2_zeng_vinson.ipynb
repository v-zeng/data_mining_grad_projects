{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/v-zeng/data_mining_grad_projects/blob/main/CISC873_DM_F21_A2_zeng_vinson.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aONsJxjVqlEZ"
      },
      "source": [
        "### **Speed Dating Match Prediction**\n",
        "\n",
        "Student: Vinson Zeng\n",
        "\n",
        "Student #: 05550960"
      ],
      "id": "aONsJxjVqlEZ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJApyLWYDy_c"
      },
      "source": [
        "‚úîÔ∏èData Science Meme\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1LtrAzDNb3yiXHF0kejLjuHx4u4gdaGAJ\">"
      ],
      "id": "rJApyLWYDy_c"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TyhfKxiruRZu"
      },
      "source": [
        "‚úîÔ∏èUnderstand the Template\n",
        "\n",
        "**What is the experimental protocol used and how was it carried out?**\n",
        "\n",
        "Predict the probability (0-1, float) that a given dating data sample will lead to a succesful match. Build a pipeline with a random forest estimator and perform hyperparameter optimization using grid search. Measure performance of model with AUROC metric. Create submission using trained model and predict successful match probability for test data.\n",
        "\n",
        "Build a second model using C-Support Vector Classification (SVC). as the estimator and optimize hyperparameters using Bayesian optimization. Score with default estimator method (SVC accuracy metric).\n",
        "\n",
        "**How did we tune hyper-parameters in the template?**\n",
        "\n",
        "Random forest classifier tuned with grid search. Exhaustive search over specified parameters in param_grid. The hyperparameters include the following:\n",
        "\n",
        "*   'preprocessor__num__imputer__strategy': ['mean']\n",
        "*   'my_classifier__n_estimators': [20, 30, 40]\n",
        "*   'my_classifier__max_depth':[10, 20, 30]\n",
        "\n",
        "GridSearchCV parameters:\n",
        "\n",
        "*  cv=3\n",
        "*  verbose=1\n",
        "*  n_jobs=2\n",
        "*  scoring='roc_auc'\n",
        "\n",
        "SVC model is tuned with Bayesian optimization using BayesSearchCV. Search spaces include the following:\n",
        "\n",
        "*  'my_svc__C': Real(1e-6, 1e+6, prior='log-uniform')\n",
        "*  'my_svc__gamma': Real(1e-6, 1e+1, prior='log-uniform')\n",
        "*  'my_svc__degree': Integer(1,8)\n",
        "*  'my_svc__kernel': Categorical(['linear', 'poly', 'rbf'])\n",
        "\n",
        "**What is the search space and what is the criteria to determine good/bad hyper-parameters?**\n",
        "\n",
        "Search space is the set of all possible solutions for the hyperparameters, or the dimensions to search for parameters of the provided estimator.\n",
        "\n",
        "For grid search, the evaluation of the combinations of hyper-parameters is given by the scoring as we defined it for the GridSearchCV parameter:\n",
        "\n",
        "*   scoring='roc_auc'\n",
        "\n",
        "AUC score closer to 1 indicates a better measure of separability (better model).\n",
        "\n",
        "\n"
      ],
      "id": "TyhfKxiruRZu"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "noYoeYRQTusR"
      },
      "source": [
        "‚úîÔ∏è Problem Formulation:\n",
        "\n",
        "**Define the problem. What is the input? What is the output?**\n",
        "\n",
        "The problem is to predict the outcome of a specific speed dating session based on the profile of two people. It is a binary classification task in which the profiles of people are the inputs and the output is the probability of a successful match.\n",
        "\n",
        "**What data mining function is required?**\n",
        "\n",
        "Classification is the data mining function required.\n",
        "\n",
        "**What could be the challenges?**\n",
        "\n",
        "Data preprocessing, such as finding the optimal method to deal with the missing data, could be a challenge. Handling the imbalanced data could also be a challenge, as there are more unmatched cases than matched.\n",
        "\n",
        "**What is the impact? What is an ideal solution?**\n",
        "\n",
        "Data preprocessing is important since it may affect the accuracy and scoring of the model. For example, if missing values are are simply removed then information will be lost.\n",
        "\n",
        "For imbalanced data, there will likely be more misclassification of the minority class (match). The majority class (unmatched) since has more data for the model to train on.\n",
        "\n",
        "An ideal solution would have an AUC score of 1, as it would mean the ability of the classifier to distinguish between classes is perfect."
      ],
      "id": "noYoeYRQTusR"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T20-xTMaQm9z"
      },
      "source": [
        "  ‚úîÔ∏è **Answer the questions below (briefly):**\n",
        "\n",
        "üåà Why a simple linear regression model (without any activation function) is not good for classification task, compared to Perceptron/Logistic regression?\n",
        "\n",
        "A linear model tries to use a straight line to describe the relationship between the input and target variables, and predicts a continuous value. The linear model is sensitive to imbalanced data and outlier observations. This is problematic since classification data is typically imbalanced and varies non-linearly with independent variables. Linear regression prioritizes minimizing the loss function at the cost of misclassifying labels correctly.  \n",
        "\n",
        "Logistic regression and perceptron models use non-linear activation functions - sigmoid and binary step, respectively. The input is transformed and are thresholded to 1 and 0 for perceptron and mapped between 0 and 1 for logistic regression. Such models should be used when target variables are discrete, such as in classification. These activation functions reduce the sensitivity of the models to outliers and transform inputs to predict discrete outcomes.\n",
        "\n",
        "üåàWhat's a decision tree and how it is different to a logistic regression model?\n",
        "\n",
        "A decision tree is a supervised machine learning method which can solve regression and classification problems. Nodes evaluate features that splits the data. The root node is the starting point which has all the training examples. Leaf nodes represent a class label. Decision trees are built recursively by evaluating metrics, such as entropy for categorical trees. Decision trees generate decision boundaries in a different manner than logistic regression. Logistic regression fits a single line (linear) while decision trees can draw multiple or non-linear boundaries to split the data points.\n",
        "\n",
        "üåàWe discussed three variants of decision tree (single-tree) in our lecture, what are their differences?\n",
        "\n",
        "Iterative Dichotomiser 3 (ID3): Uses Info Gain to decide classifying attribute, handles categorical data, does not handle missing values, is sensitive to outliers/noise, no default pruning.\n",
        "\n",
        "C4.5: Uses Gain Ratio, handles categorical and numeric data, handles missing values, is less sensitive than ID3 for outliers/noise, and uses pruning as its default setting.\n",
        "\n",
        "Classification and Regression Tree (CART): uses Gini Index, handles categorical and numeric data, handles missing values, is robust to outliers/noise, and uses pruning as its default setting.\n",
        "\n",
        "üåàWhat is the difference between the random forest model, and a bagging ensemble of CART models?\n",
        "\n",
        "Random forest uses subsets of features selected at random and takes the best feature from the subset to split each node in a tree. Bagging considers all the features for splitting a node.\n",
        "\n",
        "üåàHow is gradient boosting different to boosting (adaboost)?\n",
        "\n",
        "Gradient boosting calculates the residuals for each data point then combines them with a loss function to calculate overall loss. The loss function is differentiable, which is why gradient boosting uses gradient descent to minimize the overall loss. Adaboost assigns more weight to misclassified samples and less weight on those correctly classified. Weights affect the probability that the sample will be used to train the next weak learner."
      ],
      "id": "T20-xTMaQm9z"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1CF7CuWTpIX"
      },
      "source": [
        "**Template Code**"
      ],
      "id": "E1CF7CuWTpIX"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6bc5d7b"
      },
      "source": [
        "# load modules and functions\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import f1_score\n",
        "from pprint import pprint"
      ],
      "id": "e6bc5d7b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46ecf9fe",
        "outputId": "849dcb72-6fac-487e-968e-39acc29377dd"
      },
      "source": [
        "# read in data\n",
        "\n",
        "data = pd.read_csv('train.csv')\n",
        "data_test = pd.read_csv('test.csv')\n",
        "data.shape # return shape of the array, which is 5909 rows and 192 columns"
      ],
      "id": "46ecf9fe",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5909, 192)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "70c3bc72",
        "outputId": "552f0680-9525-4b3e-a30d-72bc2403e383"
      },
      "source": [
        "data.isnull().sum().hist() # count of null values for columns as histogram\n"
      ],
      "id": "70c3bc72",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f1449e87810>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "9e8aafce",
        "outputId": "5e54e3dd-71dc-410e-d376-db1f1a23173b"
      },
      "source": [
        " data['match'].hist() # count of 'match' column as histogram; binary, 1 is positive match"
      ],
      "id": "9e8aafce",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f1449ddd990>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tLFFQ-TRVljF"
      },
      "source": [],
      "id": "tLFFQ-TRVljF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e432518c"
      },
      "source": [
        "# down-grade scikit-learn (latest not greatest :)\n",
        "!pip install -Ivq scikit-learn==0.23.2\n",
        "\n",
        "# if you haven't installed xgboost on your system, uncomment the line below\n",
        "!pip install xgboost\n",
        "# if you haven't installed bayesian-optimization on your system, uncomment the line below\n",
        "!pip install scikit-optimize"
      ],
      "id": "e432518c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YNpG-jA_WnxH"
      },
      "source": [
        "x = data.drop('match', axis=1) # subset data as 'x' without 'match' feature\n",
        "features_numeric = list(x.select_dtypes(include=['float64'])) # subset all features with data type 'float64'\n",
        "features_categorical = list(x.select_dtypes(include=['object'])) # subset all features with data type 'object'\n",
        "y = data['match'] # the dependent feature or response variable 'match'"
      ],
      "id": "YNpG-jA_WnxH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_UMgpEqcdKB6",
        "outputId": "7d21fa70-ac1a-4adc-bb10-48fe917f54b8"
      },
      "source": [
        "print(features_categorical) # print categorical features"
      ],
      "id": "_UMgpEqcdKB6",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['field', 'undergra', 'mn_sat', 'tuition', 'from', 'zipcode', 'income', 'career']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Af-nUjlBdqAd",
        "outputId": "081776a7-a1c8-45e8-8bf3-2b756f843dc7"
      },
      "source": [
        "# load modules and functions\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import f1_score\n",
        "from pprint import pprint\n",
        "\n",
        "data = pd.read_csv('train.csv')\n",
        "data_test = pd.read_csv('test.csv')\n",
        "data.shape # return shape of the array, which is 5909 rows and 192 columns\n",
        "\n",
        "x = data.drop('match', axis=1) # subset data as 'x' without 'match' feature\n",
        "features_numeric = list(x.select_dtypes(include=['float64'])) # subset all features with data type 'float64'\n",
        "features_categorical = list(x.select_dtypes(include=['object'])) # subset all features with data type 'object'\n",
        "y = data['match'] # the dependent feature or response variable 'match'\n",
        "\n",
        "# load modules and functions\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from xgboost.sklearn import XGBClassifier\n",
        "\n",
        "np.random.seed(0) # seed the number generator\n",
        "# define steps with name and estimator object for numeric feaeture transformation pipeline\n",
        "transformer_numeric = Pipeline(\n",
        "    steps=[\n",
        "        ('imputer', SimpleImputer(strategy='median')), # replace missing values using median along each column\n",
        "        ('scaler', StandardScaler())] # standardize features by removing mean and scaling to unit variance\n",
        ")\n",
        "# define pipeline steps for categorical feature transformation\n",
        "transformer_categorical = Pipeline(\n",
        "    steps=[\n",
        "        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')), # replace missing values with 'missing'\n",
        "        ('onehot', OneHotEncoder(handle_unknown='ignore')) # columns will be all zeros if unknown categorical feature present during transform\n",
        "    ]\n",
        ")\n",
        "# specify transformers for preprocessing\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', transformer_numeric, features_numeric), # specify numeric transformer application to numeric features subset\n",
        "        ('cat', transformer_categorical, features_categorical) # specify categorical transformer application to categorical subset\n",
        "    ]\n",
        ")\n",
        "# specify full pipeline steps for preprocessing and estimator/classifier\n",
        "full_pipline = Pipeline(\n",
        "    steps=[\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('my_classifier',\n",
        "           # XGBClassifier(objective='binary:logistic', seed=1), (another example)\n",
        "           RandomForestClassifier(),\n",
        "        )\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "# `__` denotes attribute\n",
        "# (e.g. my_classifier__n_estimators means the `n_estimators` param for `my_classifier`\n",
        "#  which is our xgb)\n",
        "param_grid = {\n",
        "    'preprocessor__num__imputer__strategy': ['mean'], # try 'mean' for numeric imputer strategy\n",
        "    # 'my_classifier__n_estimators': [20, 30],  # this is for xgboost\n",
        "    # 'my_classifier__max_depth':[10, 20]       # this is for xgboost\n",
        "    'my_classifier__n_estimators': [20, 30, 40],    # this is for random forest\n",
        "    'my_classifier__max_depth':[10, 20, 30]         # this is for random forest\n",
        "}\n",
        "# instantiate GridSearchCV object with pipeline\n",
        "# cv=3: 3-fold cross validation for each combination of parameters\n",
        "# verbose=1: computation time for each fold and parameter candidate is displayed\n",
        "# n_jobs=2: run two jobs in parallel\n",
        "# use 'roc_auc' evaluation\n",
        "grid_search = GridSearchCV(\n",
        "    full_pipline, param_grid, cv=3, verbose=1, n_jobs=2,\n",
        "    scoring='roc_auc')\n",
        "\n",
        "grid_search.fit(x, y) # fit grid search to training data\n",
        "\n",
        "print('best score {}'.format(grid_search.best_score_)) # print best score\n",
        "print('best score {}'.format(grid_search.best_params_)) # print parameters associated with best score"
      ],
      "id": "Af-nUjlBdqAd",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 9 candidates, totalling 27 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  27 out of  27 | elapsed:   19.6s finished\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "best score 0.8341764456881043\n",
            "best score {'my_classifier__max_depth': 10, 'my_classifier__n_estimators': 40, 'preprocessor__num__imputer__strategy': 'mean'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 526
        },
        "id": "HImurI-qi1HR",
        "outputId": "ce13620b-3afd-4b46-997c-c2992bf6a8f4"
      },
      "source": [
        "# display cv metrics as dataframe\n",
        "print('all the cv scores')\n",
        "pd.DataFrame(grid_search.cv_results_)"
      ],
      "id": "HImurI-qi1HR",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "all the cv scores\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>mean_fit_time</th>\n",
              "      <th>std_fit_time</th>\n",
              "      <th>mean_score_time</th>\n",
              "      <th>std_score_time</th>\n",
              "      <th>param_my_classifier__max_depth</th>\n",
              "      <th>param_my_classifier__n_estimators</th>\n",
              "      <th>param_preprocessor__num__imputer__strategy</th>\n",
              "      <th>params</th>\n",
              "      <th>split0_test_score</th>\n",
              "      <th>split1_test_score</th>\n",
              "      <th>split2_test_score</th>\n",
              "      <th>mean_test_score</th>\n",
              "      <th>std_test_score</th>\n",
              "      <th>rank_test_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.518147</td>\n",
              "      <td>0.024538</td>\n",
              "      <td>0.096559</td>\n",
              "      <td>0.004064</td>\n",
              "      <td>10</td>\n",
              "      <td>20</td>\n",
              "      <td>mean</td>\n",
              "      <td>{'my_classifier__max_depth': 10, 'my_classifie...</td>\n",
              "      <td>0.843905</td>\n",
              "      <td>0.818285</td>\n",
              "      <td>0.799590</td>\n",
              "      <td>0.820594</td>\n",
              "      <td>0.018165</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.699235</td>\n",
              "      <td>0.027238</td>\n",
              "      <td>0.105055</td>\n",
              "      <td>0.004719</td>\n",
              "      <td>10</td>\n",
              "      <td>30</td>\n",
              "      <td>mean</td>\n",
              "      <td>{'my_classifier__max_depth': 10, 'my_classifie...</td>\n",
              "      <td>0.826850</td>\n",
              "      <td>0.839697</td>\n",
              "      <td>0.824648</td>\n",
              "      <td>0.830398</td>\n",
              "      <td>0.006636</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.814523</td>\n",
              "      <td>0.001191</td>\n",
              "      <td>0.114847</td>\n",
              "      <td>0.002437</td>\n",
              "      <td>10</td>\n",
              "      <td>40</td>\n",
              "      <td>mean</td>\n",
              "      <td>{'my_classifier__max_depth': 10, 'my_classifie...</td>\n",
              "      <td>0.843625</td>\n",
              "      <td>0.835194</td>\n",
              "      <td>0.823710</td>\n",
              "      <td>0.834176</td>\n",
              "      <td>0.008162</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.926312</td>\n",
              "      <td>0.021612</td>\n",
              "      <td>0.102522</td>\n",
              "      <td>0.003867</td>\n",
              "      <td>20</td>\n",
              "      <td>20</td>\n",
              "      <td>mean</td>\n",
              "      <td>{'my_classifier__max_depth': 20, 'my_classifie...</td>\n",
              "      <td>0.808621</td>\n",
              "      <td>0.818111</td>\n",
              "      <td>0.808310</td>\n",
              "      <td>0.811680</td>\n",
              "      <td>0.004549</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.296493</td>\n",
              "      <td>0.015746</td>\n",
              "      <td>0.108479</td>\n",
              "      <td>0.001099</td>\n",
              "      <td>20</td>\n",
              "      <td>30</td>\n",
              "      <td>mean</td>\n",
              "      <td>{'my_classifier__max_depth': 20, 'my_classifie...</td>\n",
              "      <td>0.830515</td>\n",
              "      <td>0.818577</td>\n",
              "      <td>0.807325</td>\n",
              "      <td>0.818806</td>\n",
              "      <td>0.009469</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1.647878</td>\n",
              "      <td>0.029070</td>\n",
              "      <td>0.123605</td>\n",
              "      <td>0.000382</td>\n",
              "      <td>20</td>\n",
              "      <td>40</td>\n",
              "      <td>mean</td>\n",
              "      <td>{'my_classifier__max_depth': 20, 'my_classifie...</td>\n",
              "      <td>0.833947</td>\n",
              "      <td>0.819010</td>\n",
              "      <td>0.819761</td>\n",
              "      <td>0.824239</td>\n",
              "      <td>0.006872</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1.071362</td>\n",
              "      <td>0.013112</td>\n",
              "      <td>0.098858</td>\n",
              "      <td>0.001230</td>\n",
              "      <td>30</td>\n",
              "      <td>20</td>\n",
              "      <td>mean</td>\n",
              "      <td>{'my_classifier__max_depth': 30, 'my_classifie...</td>\n",
              "      <td>0.807970</td>\n",
              "      <td>0.806083</td>\n",
              "      <td>0.790909</td>\n",
              "      <td>0.801654</td>\n",
              "      <td>0.007637</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1.514848</td>\n",
              "      <td>0.003019</td>\n",
              "      <td>0.112680</td>\n",
              "      <td>0.002382</td>\n",
              "      <td>30</td>\n",
              "      <td>30</td>\n",
              "      <td>mean</td>\n",
              "      <td>{'my_classifier__max_depth': 30, 'my_classifie...</td>\n",
              "      <td>0.820981</td>\n",
              "      <td>0.809568</td>\n",
              "      <td>0.799590</td>\n",
              "      <td>0.810046</td>\n",
              "      <td>0.008739</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1.848197</td>\n",
              "      <td>0.115964</td>\n",
              "      <td>0.108797</td>\n",
              "      <td>0.021048</td>\n",
              "      <td>30</td>\n",
              "      <td>40</td>\n",
              "      <td>mean</td>\n",
              "      <td>{'my_classifier__max_depth': 30, 'my_classifie...</td>\n",
              "      <td>0.826433</td>\n",
              "      <td>0.815881</td>\n",
              "      <td>0.802896</td>\n",
              "      <td>0.815070</td>\n",
              "      <td>0.009626</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   mean_fit_time  std_fit_time  ...  std_test_score  rank_test_score\n",
              "0       0.518147      0.024538  ...        0.018165                4\n",
              "1       0.699235      0.027238  ...        0.006636                2\n",
              "2       0.814523      0.001191  ...        0.008162                1\n",
              "3       0.926312      0.021612  ...        0.004549                7\n",
              "4       1.296493      0.015746  ...        0.009469                5\n",
              "5       1.647878      0.029070  ...        0.006872                3\n",
              "6       1.071362      0.013112  ...        0.007637                9\n",
              "7       1.514848      0.003019  ...        0.008739                8\n",
              "8       1.848197      0.115964  ...        0.009626                6\n",
              "\n",
              "[9 rows x 14 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "KsvKghrPqjLv",
        "outputId": "9c6410c6-b7de-464c-b6c6-5172c7fe983b"
      },
      "source": [
        "# create submission\n",
        "submission = pd.DataFrame() # create data frame\n",
        "submission['id'] = data_test['id'] # set 'id' column elements with those from 'id' in data_test\n",
        "submission['match'] = grid_search.predict_proba(data_test)[:,1] # probability of data belonging to class 1\n",
        "submission.to_csv('sample_submission_walkthrough.csv', index=False) # save data to csv file without creating index\n",
        "submission # preview data"
      ],
      "id": "KsvKghrPqjLv",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>match</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>934</td>\n",
              "      <td>0.087602</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>6539</td>\n",
              "      <td>0.293644</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>6757</td>\n",
              "      <td>0.172368</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2275</td>\n",
              "      <td>0.117874</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1052</td>\n",
              "      <td>0.090979</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2464</th>\n",
              "      <td>7982</td>\n",
              "      <td>0.165208</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2465</th>\n",
              "      <td>7299</td>\n",
              "      <td>0.290775</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2466</th>\n",
              "      <td>1818</td>\n",
              "      <td>0.086282</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2467</th>\n",
              "      <td>937</td>\n",
              "      <td>0.106396</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2468</th>\n",
              "      <td>6691</td>\n",
              "      <td>0.060121</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2469 rows √ó 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        id     match\n",
              "0      934  0.087602\n",
              "1     6539  0.293644\n",
              "2     6757  0.172368\n",
              "3     2275  0.117874\n",
              "4     1052  0.090979\n",
              "...    ...       ...\n",
              "2464  7982  0.165208\n",
              "2465  7299  0.290775\n",
              "2466  1818  0.086282\n",
              "2467   937  0.106396\n",
              "2468  6691  0.060121\n",
              "\n",
              "[2469 rows x 2 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PFcLiwDrlbxG",
        "outputId": "3a68b8a4-2891-4e73-fa6e-642e35a2e591"
      },
      "source": [
        "# import classes\n",
        "\n",
        "from skopt import BayesSearchCV\n",
        "from skopt.space import Real, Categorical, Integer\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# define SVC pipeline steps\n",
        "SVC_pipline = Pipeline(\n",
        "    steps=[\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('my_svc', SVC(class_weight='balanced'))\n",
        "    ]\n",
        ")\n",
        "# SVC has a class_weight attribute for unbalanced data\n",
        "\n",
        "# define bayesian optimization estimator and search spaces\n",
        "# search spaces contained in dictionary, where keys are parameter names and values are skopt.space.Dimension instances\n",
        "bayes_search = BayesSearchCV(\n",
        "    SVC_pipline,\n",
        "    {\n",
        "        'my_svc__C': Real(1e-6, 1e+6, prior='log-uniform'), # sample search space using 'log-uniform' distribution between ranges (optimizer will search between -6 and 6 here)\n",
        "        'my_svc__gamma': Real(1e-6, 1e+1, prior='log-uniform'), # sample search space using 'log-uniform' distribution between ranges\n",
        "        'my_svc__degree': Integer(1,8), # search space lower and upper boundary\n",
        "        'my_svc__kernel': Categorical(['linear', 'poly', 'rbf']), # category dimensions\n",
        "    },\n",
        "    n_iter=4, # number of parameter settings that are sampled\n",
        "    random_state=0, # use random numbers generated for random_state=0\n",
        "    verbose=1, # set verbosity to 1 for level of details of CV\n",
        "    cv=3, # 3-fold cross validation for each combination of parameters,\n",
        "    scoring='roc_auc'\n",
        ")\n",
        "\n",
        "bayes_search.fit(x, y) # execute bayesian optimization"
      ],
      "id": "PFcLiwDrlbxG",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:   21.6s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:   23.8s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:   22.8s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:   24.4s finished\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BayesSearchCV(cv=3,\n",
              "              estimator=Pipeline(steps=[('preprocessor',\n",
              "                                         ColumnTransformer(transformers=[('num',\n",
              "                                                                          Pipeline(steps=[('imputer',\n",
              "                                                                                           SimpleImputer(strategy='median')),\n",
              "                                                                                          ('scaler',\n",
              "                                                                                           StandardScaler())]),\n",
              "                                                                          ['positin1',\n",
              "                                                                           'pid',\n",
              "                                                                           'int_corr',\n",
              "                                                                           'age_o',\n",
              "                                                                           'race_o',\n",
              "                                                                           'pf_o_att',\n",
              "                                                                           'pf_o_sin',\n",
              "                                                                           'pf_o_int',\n",
              "                                                                           'pf_o_fun',\n",
              "                                                                           'pf_o_amb',\n",
              "                                                                           'pf_o_sha',\n",
              "                                                                           'attr_o',\n",
              "                                                                           'sinc_o',\n",
              "                                                                           'intel_o',\n",
              "                                                                           'fun_o',\n",
              "                                                                           'amb_o',\n",
              "                                                                           'sh...\n",
              "              n_iter=4, random_state=0, scoring='roc_auc',\n",
              "              search_spaces={'my_svc__C': Real(low=1e-06, high=1000000.0, prior='log-uniform', transform='normalize'),\n",
              "                             'my_svc__degree': Integer(low=1, high=8, prior='uniform', transform='normalize'),\n",
              "                             'my_svc__gamma': Real(low=1e-06, high=10.0, prior='log-uniform', transform='normalize'),\n",
              "                             'my_svc__kernel': Categorical(categories=('linear', 'poly', 'rbf'), prior=None)},\n",
              "              verbose=1)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ivDjqd_VJeFT",
        "outputId": "ac3718a5-5e30-49f7-e208-f274bd2c7462"
      },
      "source": [
        "print('best score {}'.format(bayes_search.best_score_)) # print best score from bayesian optimization\n",
        "print('best score {}'.format(bayes_search.best_params_)) # print parameters associated with best score"
      ],
      "id": "ivDjqd_VJeFT",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "best score 0.8300336031166912\n",
            "best score OrderedDict([('my_svc__C', 2.2095350994035026), ('my_svc__degree', 1), ('my_svc__gamma', 0.0002488766453161173), ('my_svc__kernel', 'linear')])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        },
        "id": "lGo26PBLJeBR",
        "outputId": "92b6f81c-20fe-415e-98db-54a1c3c7cda4"
      },
      "source": [
        "print('all the cv scores')\n",
        "pd.DataFrame(bayes_search.cv_results_) # print all cv scores"
      ],
      "id": "lGo26PBLJeBR",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "all the cv scores\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>mean_fit_time</th>\n",
              "      <th>std_fit_time</th>\n",
              "      <th>mean_score_time</th>\n",
              "      <th>std_score_time</th>\n",
              "      <th>param_my_svc__C</th>\n",
              "      <th>param_my_svc__degree</th>\n",
              "      <th>param_my_svc__gamma</th>\n",
              "      <th>param_my_svc__kernel</th>\n",
              "      <th>params</th>\n",
              "      <th>split0_test_score</th>\n",
              "      <th>split1_test_score</th>\n",
              "      <th>split2_test_score</th>\n",
              "      <th>mean_test_score</th>\n",
              "      <th>std_test_score</th>\n",
              "      <th>rank_test_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5.572630</td>\n",
              "      <td>0.838254</td>\n",
              "      <td>1.615847</td>\n",
              "      <td>0.049530</td>\n",
              "      <td>2.35272</td>\n",
              "      <td>6</td>\n",
              "      <td>0.0228543</td>\n",
              "      <td>poly</td>\n",
              "      <td>{'my_svc__C': 2.352718564818733, 'my_svc__degr...</td>\n",
              "      <td>0.713784</td>\n",
              "      <td>0.719211</td>\n",
              "      <td>0.728781</td>\n",
              "      <td>0.720592</td>\n",
              "      <td>0.006200</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>6.239062</td>\n",
              "      <td>0.752136</td>\n",
              "      <td>1.676305</td>\n",
              "      <td>0.054127</td>\n",
              "      <td>0.00126026</td>\n",
              "      <td>8</td>\n",
              "      <td>2.28596</td>\n",
              "      <td>poly</td>\n",
              "      <td>{'my_svc__C': 0.0012602593949011189, 'my_svc__...</td>\n",
              "      <td>0.699799</td>\n",
              "      <td>0.705394</td>\n",
              "      <td>0.713865</td>\n",
              "      <td>0.706353</td>\n",
              "      <td>0.005782</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>6.739534</td>\n",
              "      <td>0.250662</td>\n",
              "      <td>0.848755</td>\n",
              "      <td>0.015474</td>\n",
              "      <td>2.20954</td>\n",
              "      <td>1</td>\n",
              "      <td>0.000248877</td>\n",
              "      <td>linear</td>\n",
              "      <td>{'my_svc__C': 2.2095350994035026, 'my_svc__deg...</td>\n",
              "      <td>0.840614</td>\n",
              "      <td>0.828442</td>\n",
              "      <td>0.821045</td>\n",
              "      <td>0.830034</td>\n",
              "      <td>0.008068</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5.585071</td>\n",
              "      <td>0.011954</td>\n",
              "      <td>2.546070</td>\n",
              "      <td>0.050146</td>\n",
              "      <td>5.8754e-05</td>\n",
              "      <td>6</td>\n",
              "      <td>0.00194809</td>\n",
              "      <td>poly</td>\n",
              "      <td>{'my_svc__C': 5.87540411933884e-05, 'my_svc__d...</td>\n",
              "      <td>0.646096</td>\n",
              "      <td>0.639311</td>\n",
              "      <td>0.663252</td>\n",
              "      <td>0.649553</td>\n",
              "      <td>0.010075</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   mean_fit_time  std_fit_time  ...  std_test_score  rank_test_score\n",
              "0       5.572630      0.838254  ...        0.006200                2\n",
              "1       6.239062      0.752136  ...        0.005782                3\n",
              "2       6.739534      0.250662  ...        0.008068                1\n",
              "3       5.585071      0.011954  ...        0.010075                4\n",
              "\n",
              "[4 rows x 15 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEtdYMADUBMk"
      },
      "source": [
        "### **Trial0**\n",
        "\n",
        "The random forest classifier had a grid search score of\n",
        "0.8373225704615254. The best params are the following:\n",
        "\n",
        "\n",
        "*   'my_classifier__max_depth': 10\n",
        "*   'my_classifier__n_estimators': 40\n",
        "*   'preprocessor__num__imputer__strategy': 'mean'\n",
        "\n",
        "I will add 'most_frequent' and 'median' to the 'preprocessor__num__imputer__strategy' to see if those strategies can improve the performance.\n"
      ],
      "id": "gEtdYMADUBMk"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WGH7F1XJUA1C"
      },
      "source": [
        "#0 imputer ='most_frequent' for num and cat transformers # univariate imputation... why?\n",
        "#1 change param_grid values\n",
        "#2 multi # multivariate imputation... why?"
      ],
      "id": "WGH7F1XJUA1C",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 726
        },
        "id": "Dt6pufB0agoY",
        "outputId": "331e7240-678b-4c6b-840f-27502478d92a"
      },
      "source": [
        "### Trial0\n",
        "\n",
        "# load modules and functions\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import f1_score\n",
        "from pprint import pprint\n",
        "\n",
        "# read in data\n",
        "\n",
        "data = pd.read_csv('train.csv')\n",
        "data_test = pd.read_csv('test.csv')\n",
        "# data.shape # return shape of the array, which is 5909 rows and 192 columns\n",
        "\n",
        "# data.isnull().sum().hist() # count of null values for columns as histogram\n",
        "# data['match'].hist() # count of 'match' column as histogram; binary, 1 is positive match\n",
        "\n",
        "#======================================================\n",
        "# down-grade scikit-learn (latest not greatest :)\n",
        "!pip install -Ivq scikit-learn==0.23.2\n",
        "\n",
        "# if you haven't installed xgboost on your system, uncomment the line below\n",
        "!pip install xgboost\n",
        "# if you haven't installed bayesian-optimization on your system, uncomment the line below\n",
        "!pip install scikit-optimize\n",
        "\n",
        "#======================================================\n",
        "x = data.drop('match', axis=1) # subset data as 'x' without 'match' feature\n",
        "features_numeric = list(x.select_dtypes(include=['float64'])) # subset all features with data type 'float64'\n",
        "features_categorical = list(x.select_dtypes(include=['object'])) # subset all features with data type 'object'\n",
        "y = data['match'] # the dependent feature or response variable 'match'\n",
        "\n",
        "# print(features_categorical) # print categorical features\n",
        "\n",
        "#====================================================\n",
        "\n",
        "# load modules and functions\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from xgboost.sklearn import XGBClassifier\n",
        "\n",
        "np.random.seed(0) # seed the number generator\n",
        "# define steps with name and estimator object for numeric feaeture transformation pipeline\n",
        "transformer_numeric = Pipeline(\n",
        "    steps=[\n",
        "        ('imputer', SimpleImputer(strategy='median')), ### replace missing values using median along each column ==================\n",
        "        ('scaler', StandardScaler())] # standardize features by removing mean and scaling to unit variance\n",
        ")\n",
        "# define pipeline steps for categorical feature transformation\n",
        "transformer_categorical = Pipeline(\n",
        "    steps=[\n",
        "        ('imputer', SimpleImputer(strategy='most_frequent')), # replace missing values with most frequent ================\n",
        "        ('onehot', OneHotEncoder(handle_unknown='ignore')) # columns will be all zeros if unknown categorical feature present during transform\n",
        "    ]\n",
        ")\n",
        "# specify transformers for preprocessing\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', transformer_numeric, features_numeric), # specify numeric transformer application to numeric features subset\n",
        "        ('cat', transformer_categorical, features_categorical) # specify categorical transformer application to categorical subset\n",
        "    ]\n",
        ")\n",
        "# specify full pipeline steps for preprocessing and estimator/classifier\n",
        "full_pipline = Pipeline(\n",
        "    steps=[\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('my_classifier',\n",
        "           # XGBClassifier(objective='binary:logistic', seed=1), (another example)\n",
        "           RandomForestClassifier(),\n",
        "        )\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "# `__` denotes attribute\n",
        "# (e.g. my_classifier__n_estimators means the `n_estimators` param for `my_classifier`\n",
        "#  which is our xgb)\n",
        "param_grid = {\n",
        "    'preprocessor__num__imputer__strategy': ['median','most_frequent','mean'], # this is for random forest\n",
        "    # 'my_classifier__n_estimators': [20, 30],  # this is for xgboost\n",
        "    # 'my_classifier__max_depth':[10, 20]       # this is for xgboost\n",
        "    'my_classifier__n_estimators': [20, 30, 40],    # this is for random forest ======== best: 40\n",
        "    'my_classifier__max_depth':[10, 20, 30]         # this is for random forest ======= best: 10\n",
        "}\n",
        "# instantiate GridSearchCV object with pipeline\n",
        "# cv=3: 3-fold cross validation for each combination of parameters\n",
        "# verbose=1: computation time for each fold and parameter candidate is displayed\n",
        "# n_jobs=2: run two jobs in parallel\n",
        "# use 'roc_auc' evaluation\n",
        "grid_search = GridSearchCV(\n",
        "    full_pipline, param_grid, cv=3, verbose=1, n_jobs=2,\n",
        "    scoring='roc_auc')\n",
        "\n",
        "grid_search.fit(x, y) # fit grid search to training data\n",
        "\n",
        "print('grid best score {}'.format(grid_search.best_score_)) # print best score\n",
        "print('grid best params {}'.format(grid_search.best_params_)) # print parameters associated with best score\n",
        "\n"
      ],
      "id": "Dt6pufB0agoY",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikit-learn==0.23.2\n",
            "  Using cached scikit_learn-0.23.2-cp37-cp37m-manylinux1_x86_64.whl (6.8 MB)\n",
            "Collecting scipy>=0.19.1\n",
            "  Using cached scipy-1.7.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (28.5 MB)\n",
            "Collecting threadpoolctl>=2.0.0\n",
            "  Using cached threadpoolctl-3.0.0-py3-none-any.whl (14 kB)\n",
            "Collecting numpy>=1.13.3\n",
            "  Using cached numpy-1.21.3-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
            "Collecting joblib>=0.11\n",
            "  Using cached joblib-1.1.0-py2.py3-none-any.whl (306 kB)\n",
            "Installing collected packages: numpy, threadpoolctl, scipy, joblib, scikit-learn\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.6.0 requires numpy~=1.19.2, but you have numpy 1.21.3 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed joblib-1.1.0 numpy-1.21.3 scikit-learn-0.23.2 scipy-1.7.1 threadpoolctl-3.0.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "joblib",
                  "numpy",
                  "scipy",
                  "sklearn"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.7/dist-packages (0.90)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from xgboost) (1.7.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from xgboost) (1.21.3)\n",
            "Requirement already satisfied: scikit-optimize in /usr/local/lib/python3.7/dist-packages (0.9.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scikit-optimize) (1.21.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-optimize) (1.1.0)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.7/dist-packages (from scikit-optimize) (0.23.2)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit-optimize) (1.7.1)\n",
            "Requirement already satisfied: pyaml>=16.9 in /usr/local/lib/python3.7/dist-packages (from scikit-optimize) (21.10.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from pyaml>=16.9->scikit-optimize) (3.13)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->scikit-optimize) (3.0.0)\n",
            "Fitting 3 folds for each of 27 candidates, totalling 81 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:   20.4s\n",
            "[Parallel(n_jobs=2)]: Done  81 out of  81 | elapsed:   45.6s finished\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "grid best score 0.8396058232204822\n",
            "grid best params {'my_classifier__max_depth': 10, 'my_classifier__n_estimators': 40, 'preprocessor__num__imputer__strategy': 'mean'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "nYvMzQ6z8BBg",
        "outputId": "729de546-3878-4069-9761-0071fbb704e6"
      },
      "source": [
        "# create submission\n",
        "submission = pd.DataFrame() # create data frame\n",
        "submission['id'] = data_test['id'] # set 'id' column elements with those from 'id' in data_test\n",
        "submission['match'] = grid_search.predict_proba(data_test)[:,1] # probability of data belonging to class 1\n",
        "submission.to_csv('submission_trial0.csv', index=False) # save data to csv file without creating index\n",
        "submission # preview data"
      ],
      "id": "nYvMzQ6z8BBg",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>match</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>934</td>\n",
              "      <td>0.087981</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>6539</td>\n",
              "      <td>0.248986</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>6757</td>\n",
              "      <td>0.209547</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2275</td>\n",
              "      <td>0.082276</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1052</td>\n",
              "      <td>0.119962</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2464</th>\n",
              "      <td>7982</td>\n",
              "      <td>0.195369</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2465</th>\n",
              "      <td>7299</td>\n",
              "      <td>0.377913</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2466</th>\n",
              "      <td>1818</td>\n",
              "      <td>0.098586</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2467</th>\n",
              "      <td>937</td>\n",
              "      <td>0.155603</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2468</th>\n",
              "      <td>6691</td>\n",
              "      <td>0.089616</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2469 rows √ó 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        id     match\n",
              "0      934  0.087981\n",
              "1     6539  0.248986\n",
              "2     6757  0.209547\n",
              "3     2275  0.082276\n",
              "4     1052  0.119962\n",
              "...    ...       ...\n",
              "2464  7982  0.195369\n",
              "2465  7299  0.377913\n",
              "2466  1818  0.098586\n",
              "2467   937  0.155603\n",
              "2468  6691  0.089616\n",
              "\n",
              "[2469 rows x 2 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uPySsUMAQMu"
      },
      "source": [
        "### **Trial1**\n",
        "\n",
        "In trial0 the auc score was similar to the template trial score. THe best params did not change either.\n",
        "\n",
        "**Trial0 grid best score 0.8385807930856641**\n",
        "\n",
        "**Trial0 grid best params** {'my_classifier__max_depth': 10, 'my_classifier__n_estimators': 40, 'preprocessor__num__imputer__strategy': 'mean'}\n",
        "\n",
        "In trial1 I will set the following parameters in param_grid:\n",
        "\n",
        "*   'my_classifier__n_estimators': [40, 50, 60]\n",
        "*   'my_classifier__max_depth':[5, 10, 20]\n",
        "\n",
        "The best params are at the respective extremes of the listed parameters, which means the optimal value may lie beyond. I predict that there will be a more optimal value for at least one of the parameters in the ones listed."
      ],
      "id": "9uPySsUMAQMu"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 726
        },
        "id": "6nFIe4x4AMPN",
        "outputId": "3be48aa9-d86f-456d-ae1c-89177ef63cea"
      },
      "source": [
        "# Trial1\n",
        "\n",
        "\n",
        "### Test Code ============================================\n",
        "\n",
        "# load modules and functions\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import f1_score\n",
        "from pprint import pprint\n",
        "\n",
        "# read in data\n",
        "\n",
        "data = pd.read_csv('train.csv')\n",
        "data_test = pd.read_csv('test.csv')\n",
        "# data.shape # return shape of the array, which is 5909 rows and 192 columns\n",
        "\n",
        "# data.isnull().sum().hist() # count of null values for columns as histogram\n",
        "# data['match'].hist() # count of 'match' column as histogram; binary, 1 is positive match\n",
        "\n",
        "#======================================================\n",
        "# down-grade scikit-learn (latest not greatest :)\n",
        "!pip install -Ivq scikit-learn==0.23.2\n",
        "\n",
        "# if you haven't installed xgboost on your system, uncomment the line below\n",
        "!pip install xgboost\n",
        "# if you haven't installed bayesian-optimization on your system, uncomment the line below\n",
        "!pip install scikit-optimize\n",
        "\n",
        "#======================================================\n",
        "x = data.drop('match', axis=1) # subset data as 'x' without 'match' feature\n",
        "features_numeric = list(x.select_dtypes(include=['float64'])) # subset all features with data type 'float64'\n",
        "features_categorical = list(x.select_dtypes(include=['object'])) # subset all features with data type 'object'\n",
        "y = data['match'] # the dependent feature or response variable 'match'\n",
        "\n",
        "# print(features_categorical) # print categorical features\n",
        "\n",
        "#====================================================\n",
        "\n",
        "# load modules and functions\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "# explicitly require this experimental feature\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "# now you can import normally from sklearn.impute\n",
        "from sklearn.impute import IterativeImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from xgboost.sklearn import XGBClassifier\n",
        "from sklearn.linear_model import BayesianRidge\n",
        "from sklearn.ensemble import ExtraTreesRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.impute import KNNImputer\n",
        "\n",
        "np.random.seed(0) # seed the number generator\n",
        "# define steps with name and estimator object for numeric feaeture transformation pipeline\n",
        "transformer_numeric = Pipeline(\n",
        "    steps=[\n",
        "        ('imputer', SimpleImputer(strategy='median')), ### replace missing values using median along each column ==================\n",
        "        ('scaler', StandardScaler())] # standardize features by removing mean and scaling to unit variance\n",
        ")\n",
        "# define pipeline steps for categorical feature transformation\n",
        "transformer_categorical = Pipeline(\n",
        "    steps=[\n",
        "        ('imputer', SimpleImputer(strategy='most_frequent')), # replace missing values with most frequent ================\n",
        "        ('onehot', OneHotEncoder(handle_unknown='ignore')) # columns will be all zeros if unknown categorical feature present during transform\n",
        "    ]\n",
        ")\n",
        "# specify transformers for preprocessing\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', transformer_numeric, features_numeric), # specify numeric transformer application to numeric features subset\n",
        "        ('cat', transformer_categorical, features_categorical) # specify categorical transformer application to categorical subset\n",
        "    ]\n",
        ")\n",
        "# specify full pipeline steps for preprocessing and estimator/classifier\n",
        "full_pipline = Pipeline(\n",
        "    steps=[\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('my_classifier',\n",
        "           # XGBClassifier(objective='binary:logistic', seed=1), (another example)\n",
        "           RandomForestClassifier(),\n",
        "        )\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "# `__` denotes attribute\n",
        "# (e.g. my_classifier__n_estimators means the `n_estimators` param for `my_classifier`\n",
        "#  which is our xgb)\n",
        "param_grid = {\n",
        "    'preprocessor__num__imputer__strategy': ['median','most_frequent','mean'], # this is for random forest\n",
        "    # 'my_classifier__n_estimators': [20, 30],  # this is for xgboost\n",
        "    # 'my_classifier__max_depth':[10, 20]       # this is for xgboost\n",
        "    'my_classifier__n_estimators': [40, 50, 60],    # this is for random forest ======== best: 40, remove 20, 30 and add 50, 60\n",
        "    'my_classifier__max_depth':[5, 10, 20]         # this is for random forest ======= best: 10, remove 30 and add 5\n",
        "}\n",
        "# instantiate GridSearchCV object with pipeline\n",
        "# cv=3: 3-fold cross validation for each combination of parameters\n",
        "# verbose=1: computation time for each fold and parameter candidate is displayed\n",
        "# n_jobs=2: run two jobs in parallel\n",
        "# use 'roc_auc' evaluation\n",
        "grid_search = GridSearchCV(\n",
        "    full_pipline, param_grid, cv=3, verbose=1, n_jobs=2,\n",
        "    scoring='roc_auc')\n",
        "\n",
        "grid_search.fit(x, y) # fit grid search to training data\n",
        "\n",
        "print('grid best score {}'.format(grid_search.best_score_)) # print best score\n",
        "print('grid best params {}'.format(grid_search.best_params_)) # print parameters associated with best score\n",
        "\n"
      ],
      "id": "6nFIe4x4AMPN",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikit-learn==0.23.2\n",
            "  Using cached scikit_learn-0.23.2-cp37-cp37m-manylinux1_x86_64.whl (6.8 MB)\n",
            "Collecting threadpoolctl>=2.0.0\n",
            "  Using cached threadpoolctl-3.0.0-py3-none-any.whl (14 kB)\n",
            "Collecting scipy>=0.19.1\n",
            "  Using cached scipy-1.7.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (28.5 MB)\n",
            "Collecting joblib>=0.11\n",
            "  Using cached joblib-1.1.0-py2.py3-none-any.whl (306 kB)\n",
            "Collecting numpy>=1.13.3\n",
            "  Using cached numpy-1.21.3-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
            "Installing collected packages: numpy, threadpoolctl, scipy, joblib, scikit-learn\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.6.0 requires numpy~=1.19.2, but you have numpy 1.21.3 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed joblib-1.1.0 numpy-1.21.3 scikit-learn-0.23.2 scipy-1.7.1 threadpoolctl-3.0.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "joblib",
                  "numpy",
                  "scipy",
                  "sklearn"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.7/dist-packages (0.90)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from xgboost) (1.7.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from xgboost) (1.21.3)\n",
            "Requirement already satisfied: scikit-optimize in /usr/local/lib/python3.7/dist-packages (0.9.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-optimize) (1.1.0)\n",
            "Requirement already satisfied: pyaml>=16.9 in /usr/local/lib/python3.7/dist-packages (from scikit-optimize) (21.10.1)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit-optimize) (1.7.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.7/dist-packages (from scikit-optimize) (0.23.2)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scikit-optimize) (1.21.3)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from pyaml>=16.9->scikit-optimize) (3.13)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->scikit-optimize) (3.0.0)\n",
            "Fitting 3 folds for each of 27 candidates, totalling 81 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:   17.5s\n",
            "[Parallel(n_jobs=2)]: Done  81 out of  81 | elapsed:   46.8s finished\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "grid best score 0.8432482330272656\n",
            "grid best params {'my_classifier__max_depth': 10, 'my_classifier__n_estimators': 50, 'preprocessor__num__imputer__strategy': 'most_frequent'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "yPOCrhp27e4U",
        "outputId": "9e5b9afd-0046-4e41-ae9c-e43764a5351c"
      },
      "source": [
        "# create submission\n",
        "submission = pd.DataFrame() # create data frame\n",
        "submission['id'] = data_test['id'] # set 'id' column elements with those from 'id' in data_test\n",
        "submission['match'] = grid_search.predict_proba(data_test)[:,1] # probability of data belonging to class 1\n",
        "submission.to_csv('submission_trial1.csv', index=False) # save data to csv file without creating index\n",
        "submission # preview data"
      ],
      "id": "yPOCrhp27e4U",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>match</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>934</td>\n",
              "      <td>0.094905</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>6539</td>\n",
              "      <td>0.188265</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>6757</td>\n",
              "      <td>0.158770</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2275</td>\n",
              "      <td>0.093993</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1052</td>\n",
              "      <td>0.140613</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2464</th>\n",
              "      <td>7982</td>\n",
              "      <td>0.171842</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2465</th>\n",
              "      <td>7299</td>\n",
              "      <td>0.327227</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2466</th>\n",
              "      <td>1818</td>\n",
              "      <td>0.089098</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2467</th>\n",
              "      <td>937</td>\n",
              "      <td>0.106191</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2468</th>\n",
              "      <td>6691</td>\n",
              "      <td>0.123649</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2469 rows √ó 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        id     match\n",
              "0      934  0.094905\n",
              "1     6539  0.188265\n",
              "2     6757  0.158770\n",
              "3     2275  0.093993\n",
              "4     1052  0.140613\n",
              "...    ...       ...\n",
              "2464  7982  0.171842\n",
              "2465  7299  0.327227\n",
              "2466  1818  0.089098\n",
              "2467   937  0.106191\n",
              "2468  6691  0.123649\n",
              "\n",
              "[2469 rows x 2 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96II6khFZYyc"
      },
      "source": [
        "###**Trial2**\n",
        "The score seems to have improved a bit from trial1 and the best parameters have further changed.\n",
        "\n",
        "**grid best score 0.8429941903780263**\n",
        "\n",
        "**grid best params**\n",
        "{'my_classifier__max_depth': 5, 'my_classifier__n_estimators': 60, 'preprocessor__num__imputer__strategy': 'most_frequent'}\n",
        "\n",
        "I will try changing the range of the parameters again relative to the results. In trial2 I will set the following parameters in param_grid:\n",
        "\n",
        "*   'my_classifier__n_estimators': [60, 70, 80]\n",
        "*   'my_classifier__max_depth':[1, 5, 10]\n",
        "\n",
        "I predict the best params will change again as the previous trial showed the params were at the limiting range."
      ],
      "id": "96II6khFZYyc"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yEthoxUcPLFK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 726
        },
        "outputId": "dbbc5c47-116a-4566-9d9c-782550fbdf60"
      },
      "source": [
        "# Trial2\n",
        "\n",
        "\n",
        "### Test Code ============================================\n",
        "\n",
        "# load modules and functions\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import f1_score\n",
        "from pprint import pprint\n",
        "\n",
        "# read in data\n",
        "\n",
        "data = pd.read_csv('train.csv')\n",
        "data_test = pd.read_csv('test.csv')\n",
        "# data.shape # return shape of the array, which is 5909 rows and 192 columns\n",
        "\n",
        "# data.isnull().sum().hist() # count of null values for columns as histogram\n",
        "# data['match'].hist() # count of 'match' column as histogram; binary, 1 is positive match\n",
        "\n",
        "#======================================================\n",
        "# down-grade scikit-learn (latest not greatest :)\n",
        "!pip install -Ivq scikit-learn==0.23.2\n",
        "\n",
        "# if you haven't installed xgboost on your system, uncomment the line below\n",
        "!pip install xgboost\n",
        "# if you haven't installed bayesian-optimization on your system, uncomment the line below\n",
        "!pip install scikit-optimize\n",
        "\n",
        "#======================================================\n",
        "x = data.drop('match', axis=1) # subset data as 'x' without 'match' feature\n",
        "features_numeric = list(x.select_dtypes(include=['float64'])) # subset all features with data type 'float64'\n",
        "features_categorical = list(x.select_dtypes(include=['object'])) # subset all features with data type 'object'\n",
        "y = data['match'] # the dependent feature or response variable 'match'\n",
        "\n",
        "# print(features_categorical) # print categorical features\n",
        "\n",
        "#====================================================\n",
        "\n",
        "# load modules and functions\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "# explicitly require this experimental feature\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "# now you can import normally from sklearn.impute\n",
        "from sklearn.impute import IterativeImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from xgboost.sklearn import XGBClassifier\n",
        "from sklearn.linear_model import BayesianRidge\n",
        "from sklearn.ensemble import ExtraTreesRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.impute import KNNImputer\n",
        "\n",
        "np.random.seed(0) # seed the number generator\n",
        "# define steps with name and estimator object for numeric feaeture transformation pipeline\n",
        "transformer_numeric = Pipeline(\n",
        "    steps=[\n",
        "        ('imputer', SimpleImputer(strategy='median')), ### replace missing values using median along each column ==================\n",
        "        ('scaler', StandardScaler())] # standardize features by removing mean and scaling to unit variance\n",
        ")\n",
        "# define pipeline steps for categorical feature transformation\n",
        "transformer_categorical = Pipeline(\n",
        "    steps=[\n",
        "        ('imputer', SimpleImputer(strategy='most_frequent')), # replace missing values with most frequent ================\n",
        "        ('onehot', OneHotEncoder(handle_unknown='ignore')) # columns will be all zeros if unknown categorical feature present during transform\n",
        "    ]\n",
        ")\n",
        "# specify transformers for preprocessing\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', transformer_numeric, features_numeric), # specify numeric transformer application to numeric features subset\n",
        "        ('cat', transformer_categorical, features_categorical) # specify categorical transformer application to categorical subset\n",
        "    ]\n",
        ")\n",
        "# specify full pipeline steps for preprocessing and estimator/classifier\n",
        "full_pipline = Pipeline(\n",
        "    steps=[\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('my_classifier',\n",
        "           # XGBClassifier(objective='binary:logistic', seed=1), (another example)\n",
        "           RandomForestClassifier(),\n",
        "        )\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "# `__` denotes attribute\n",
        "# (e.g. my_classifier__n_estimators means the `n_estimators` param for `my_classifier`\n",
        "#  which is our xgb)\n",
        "param_grid = {\n",
        "    'preprocessor__num__imputer__strategy': ['median','most_frequent','mean'], # this is for random forest\n",
        "    # 'my_classifier__n_estimators': [20, 30],  # this is for xgboost\n",
        "    # 'my_classifier__max_depth':[10, 20]       # this is for xgboost\n",
        "    'my_classifier__n_estimators': [60, 70, 80],    # this is for random forest ======== best: 40, remove 20, 30 and add 50, 60\n",
        "    'my_classifier__max_depth':[1, 5, 10]         # this is for random forest ======= best: 10, remove 30 and add 5\n",
        "}\n",
        "# instantiate GridSearchCV object with pipeline\n",
        "# cv=3: 3-fold cross validation for each combination of parameters\n",
        "# verbose=1: computation time for each fold and parameter candidate is displayed\n",
        "# n_jobs=2: run two jobs in parallel\n",
        "# use 'roc_auc' evaluation\n",
        "grid_search = GridSearchCV(\n",
        "    full_pipline, param_grid, cv=3, verbose=1, n_jobs=2,\n",
        "    scoring='roc_auc')\n",
        "\n",
        "grid_search.fit(x, y) # fit grid search to training data\n",
        "\n",
        "print('grid best score {}'.format(grid_search.best_score_)) # print best score\n",
        "print('grid best params {}'.format(grid_search.best_params_)) # print parameters associated with best score\n"
      ],
      "id": "yEthoxUcPLFK",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikit-learn==0.23.2\n",
            "  Using cached scikit_learn-0.23.2-cp37-cp37m-manylinux1_x86_64.whl (6.8 MB)\n",
            "Collecting scipy>=0.19.1\n",
            "  Using cached scipy-1.7.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (28.5 MB)\n",
            "Collecting threadpoolctl>=2.0.0\n",
            "  Using cached threadpoolctl-3.0.0-py3-none-any.whl (14 kB)\n",
            "Collecting numpy>=1.13.3\n",
            "  Using cached numpy-1.21.3-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
            "Collecting joblib>=0.11\n",
            "  Using cached joblib-1.1.0-py2.py3-none-any.whl (306 kB)\n",
            "Installing collected packages: numpy, threadpoolctl, scipy, joblib, scikit-learn\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.6.0 requires numpy~=1.19.2, but you have numpy 1.21.3 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed joblib-1.1.0 numpy-1.21.3 scikit-learn-0.23.2 scipy-1.7.1 threadpoolctl-3.0.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "joblib",
                  "numpy",
                  "scipy",
                  "sklearn"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.7/dist-packages (0.90)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from xgboost) (1.7.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from xgboost) (1.21.3)\n",
            "Requirement already satisfied: scikit-optimize in /usr/local/lib/python3.7/dist-packages (0.9.0)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit-optimize) (1.7.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-optimize) (1.1.0)\n",
            "Requirement already satisfied: pyaml>=16.9 in /usr/local/lib/python3.7/dist-packages (from scikit-optimize) (21.10.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.7/dist-packages (from scikit-optimize) (0.23.2)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scikit-optimize) (1.21.3)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from pyaml>=16.9->scikit-optimize) (3.13)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->scikit-optimize) (3.0.0)\n",
            "Fitting 3 folds for each of 27 candidates, totalling 81 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:   14.5s\n",
            "[Parallel(n_jobs=2)]: Done  81 out of  81 | elapsed:   33.8s finished\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "grid best score 0.84601975474231\n",
            "grid best params {'my_classifier__max_depth': 10, 'my_classifier__n_estimators': 80, 'preprocessor__num__imputer__strategy': 'mean'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "W1MD0vQY9ZVO",
        "outputId": "55921110-5862-43c1-ae09-82012742d8ca"
      },
      "source": [
        "# create submission\n",
        "submission = pd.DataFrame() # create data frame\n",
        "submission['id'] = data_test['id'] # set 'id' column elements with those from 'id' in data_test\n",
        "submission['match'] = grid_search.predict_proba(data_test)[:,1] # probability of data belonging to class 1\n",
        "submission.to_csv('submission_trial2.csv', index=False) # save data to csv file without creating index\n",
        "submission # preview data"
      ],
      "id": "W1MD0vQY9ZVO",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>match</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>934</td>\n",
              "      <td>0.089907</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>6539</td>\n",
              "      <td>0.217972</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>6757</td>\n",
              "      <td>0.198676</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2275</td>\n",
              "      <td>0.102680</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1052</td>\n",
              "      <td>0.123625</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2464</th>\n",
              "      <td>7982</td>\n",
              "      <td>0.192284</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2465</th>\n",
              "      <td>7299</td>\n",
              "      <td>0.380262</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2466</th>\n",
              "      <td>1818</td>\n",
              "      <td>0.101737</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2467</th>\n",
              "      <td>937</td>\n",
              "      <td>0.144713</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2468</th>\n",
              "      <td>6691</td>\n",
              "      <td>0.092936</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2469 rows √ó 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        id     match\n",
              "0      934  0.089907\n",
              "1     6539  0.217972\n",
              "2     6757  0.198676\n",
              "3     2275  0.102680\n",
              "4     1052  0.123625\n",
              "...    ...       ...\n",
              "2464  7982  0.192284\n",
              "2465  7299  0.380262\n",
              "2466  1818  0.101737\n",
              "2467   937  0.144713\n",
              "2468  6691  0.092936\n",
              "\n",
              "[2469 rows x 2 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oy1q3v4SDxJ"
      },
      "source": [
        "###**Trial3**\n",
        "\n",
        "The score seems to be very similar to the previous trial, but the best param for 'my_classifier_n_estimators' has changed to 80.\n",
        "\n",
        "**grid best score 0.8437989568944776**\n",
        "\n",
        "**grid best params** {'my_classifier__max_depth': 10, 'my_classifier__n_estimators': 80, 'preprocessor__num__imputer__strategy': 'mean'}\n",
        "\n",
        "**submission score 0.85309**\n",
        "\n",
        "Sumbmission score is slightly better, perhaps the data could use some better preprocessing.\n",
        "\n",
        "In this trial I will try using KNN Imputer to see if that improves the model performance. I assume that KNN Imputer will provide more accurate estimates of missing values since it is a multivariate imputer."
      ],
      "id": "7oy1q3v4SDxJ"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 708
        },
        "id": "GktRKDMuSB7G",
        "outputId": "73cf6d81-6792-4013-9bb6-1faa27d4d76e"
      },
      "source": [
        "# Trial3\n",
        "\n",
        "\n",
        "### Test Code ============================================\n",
        "\n",
        "# load modules and functions\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import f1_score\n",
        "from pprint import pprint\n",
        "\n",
        "# read in data\n",
        "\n",
        "data = pd.read_csv('train.csv')\n",
        "data_test = pd.read_csv('test.csv')\n",
        "# data.shape # return shape of the array, which is 5909 rows and 192 columns\n",
        "\n",
        "# data.isnull().sum().hist() # count of null values for columns as histogram\n",
        "# data['match'].hist() # count of 'match' column as histogram; binary, 1 is positive match\n",
        "\n",
        "#======================================================\n",
        "# down-grade scikit-learn (latest not greatest :)\n",
        "!pip install -Ivq scikit-learn==0.23.2\n",
        "\n",
        "# if you haven't installed xgboost on your system, uncomment the line below\n",
        "!pip install xgboost\n",
        "# if you haven't installed bayesian-optimization on your system, uncomment the line below\n",
        "!pip install scikit-optimize\n",
        "\n",
        "#======================================================\n",
        "x = data.drop('match', axis=1) # subset data as 'x' without 'match' feature\n",
        "features_numeric = list(x.select_dtypes(include=['float64'])) # subset all features with data type 'float64'\n",
        "features_categorical = list(x.select_dtypes(include=['object'])) # subset all features with data type 'object'\n",
        "y = data['match'] # the dependent feature or response variable 'match'\n",
        "\n",
        "# print(features_categorical) # print categorical features\n",
        "\n",
        "#====================================================\n",
        "\n",
        "# load modules and functions\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "# explicitly require this experimental feature\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "# now you can import normally from sklearn.impute\n",
        "from sklearn.impute import IterativeImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from xgboost.sklearn import XGBClassifier\n",
        "from sklearn.linear_model import BayesianRidge\n",
        "from sklearn.ensemble import ExtraTreesRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.impute import KNNImputer\n",
        "\n",
        "np.random.seed(0) # seed the number generator\n",
        "# define steps with name and estimator object for numeric feaeture transformation pipeline\n",
        "transformer_numeric = Pipeline(\n",
        "    steps=[\n",
        "        ('imputer', KNNImputer()), ### imputation using k-Nearest Neighbors\n",
        "        ('scaler', StandardScaler())] # standardize features by removing mean and scaling to unit variance\n",
        ")\n",
        "# define pipeline steps for categorical feature transformation\n",
        "transformer_categorical = Pipeline(\n",
        "    steps=[\n",
        "        ('imputer', SimpleImputer(strategy='most_frequent')), # replace missing values with most frequent\n",
        "        ('onehot', OneHotEncoder(handle_unknown='ignore')) # columns will be all zeros if unknown categorical feature present during transform\n",
        "    ]\n",
        ")\n",
        "# specify transformers for preprocessing\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', transformer_numeric, features_numeric), # specify numeric transformer application to numeric features subset\n",
        "        ('cat', transformer_categorical, features_categorical) # specify categorical transformer application to categorical subset\n",
        "    ]\n",
        ")\n",
        "# specify full pipeline steps for preprocessing and estimator/classifier\n",
        "full_pipline = Pipeline(\n",
        "    steps=[\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('my_classifier',\n",
        "           # XGBClassifier(objective='binary:logistic', seed=1), (another example)\n",
        "           RandomForestClassifier(),\n",
        "        )\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "# `__` denotes attribute\n",
        "# (e.g. my_classifier__n_estimators means the `n_estimators` param for `my_classifier`\n",
        "#  which is our xgb)\n",
        "param_grid = {\n",
        "    # 'preprocessor__num__imputer__strategy': ['median','most_frequent','mean'], # this is for random forest\n",
        "    # 'my_classifier__n_estimators': [20, 30],  # this is for xgboost\n",
        "    # 'my_classifier__max_depth':[10, 20]       # this is for xgboost\n",
        "    'my_classifier__n_estimators': [60, 70, 80],    # this is for random forest ======== best: 40, remove 20, 30 and add 50, 60\n",
        "    'my_classifier__max_depth':[1, 5, 10]         # this is for random forest ======= best: 10, remove 30 and add 5\n",
        "}\n",
        "# instantiate GridSearchCV object with pipeline\n",
        "# cv=3: 3-fold cross validation for each combination of parameters\n",
        "# verbose=1: computation time for each fold and parameter candidate is displayed\n",
        "# n_jobs=2: run two jobs in parallel\n",
        "# use 'roc_auc' evaluation\n",
        "grid_search = GridSearchCV(\n",
        "    full_pipline, param_grid, cv=3, verbose=1, n_jobs=2,\n",
        "    scoring='roc_auc')\n",
        "\n",
        "grid_search.fit(x, y) # fit grid search to training data\n",
        "\n",
        "print('grid best score {}'.format(grid_search.best_score_)) # print best score\n",
        "print('grid best params {}'.format(grid_search.best_params_)) # print parameters associated with best score\n",
        "\n"
      ],
      "id": "GktRKDMuSB7G",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikit-learn==0.23.2\n",
            "  Using cached scikit_learn-0.23.2-cp37-cp37m-manylinux1_x86_64.whl (6.8 MB)\n",
            "Collecting numpy>=1.13.3\n",
            "  Using cached numpy-1.21.3-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
            "Collecting scipy>=0.19.1\n",
            "  Using cached scipy-1.7.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (28.5 MB)\n",
            "Collecting threadpoolctl>=2.0.0\n",
            "  Using cached threadpoolctl-3.0.0-py3-none-any.whl (14 kB)\n",
            "Collecting joblib>=0.11\n",
            "  Using cached joblib-1.1.0-py2.py3-none-any.whl (306 kB)\n",
            "Installing collected packages: numpy, threadpoolctl, scipy, joblib, scikit-learn\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.6.0 requires numpy~=1.19.2, but you have numpy 1.21.3 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed joblib-1.1.0 numpy-1.21.3 scikit-learn-0.23.2 scipy-1.7.1 threadpoolctl-3.0.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "joblib",
                  "numpy",
                  "scipy",
                  "sklearn"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.7/dist-packages (0.90)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from xgboost) (1.7.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from xgboost) (1.21.3)\n",
            "Requirement already satisfied: scikit-optimize in /usr/local/lib/python3.7/dist-packages (0.9.0)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit-optimize) (1.7.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-optimize) (1.1.0)\n",
            "Requirement already satisfied: pyaml>=16.9 in /usr/local/lib/python3.7/dist-packages (from scikit-optimize) (21.10.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scikit-optimize) (1.21.3)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.7/dist-packages (from scikit-optimize) (0.23.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from pyaml>=16.9->scikit-optimize) (3.13)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->scikit-optimize) (3.0.0)\n",
            "Fitting 3 folds for each of 9 candidates, totalling 27 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  27 out of  27 | elapsed:  5.8min finished\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "grid best score 0.8367571464557857\n",
            "grid best params {'my_classifier__max_depth': 10, 'my_classifier__n_estimators': 70}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "S4In-yMf_IMR",
        "outputId": "ad6922f5-5555-481f-ba5a-541ce8227f21"
      },
      "source": [
        "# create submission\n",
        "submission = pd.DataFrame() # create data frame\n",
        "submission['id'] = data_test['id'] # set 'id' column elements with those from 'id' in data_test\n",
        "submission['match'] = grid_search.predict_proba(data_test)[:,1] # probability of data belonging to class 1\n",
        "submission.to_csv('submission_trial3.csv', index=False) # save data to csv file without creating index\n",
        "submission # preview data"
      ],
      "id": "S4In-yMf_IMR",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>match</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>934</td>\n",
              "      <td>0.111354</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>6539</td>\n",
              "      <td>0.221732</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>6757</td>\n",
              "      <td>0.154822</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2275</td>\n",
              "      <td>0.078800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1052</td>\n",
              "      <td>0.113483</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2464</th>\n",
              "      <td>7982</td>\n",
              "      <td>0.179407</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2465</th>\n",
              "      <td>7299</td>\n",
              "      <td>0.386794</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2466</th>\n",
              "      <td>1818</td>\n",
              "      <td>0.080358</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2467</th>\n",
              "      <td>937</td>\n",
              "      <td>0.113698</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2468</th>\n",
              "      <td>6691</td>\n",
              "      <td>0.099646</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2469 rows √ó 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        id     match\n",
              "0      934  0.111354\n",
              "1     6539  0.221732\n",
              "2     6757  0.154822\n",
              "3     2275  0.078800\n",
              "4     1052  0.113483\n",
              "...    ...       ...\n",
              "2464  7982  0.179407\n",
              "2465  7299  0.386794\n",
              "2466  1818  0.080358\n",
              "2467   937  0.113698\n",
              "2468  6691  0.099646\n",
              "\n",
              "[2469 rows x 2 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qoLG-VvZXqM"
      },
      "source": [
        "### **Trial4**\n",
        "\n",
        "The score using KNN Imputation is similar to the previous trials, it has not improved. Slightly worse on the test score.\n",
        "\n",
        "**grid best score 0.8335425432335146**\n",
        "\n",
        "**grid best params** {'my_classifier__max_depth': 5, 'my_classifier__n_estimators': 70}\n",
        "\n",
        "**submission scored 0.84416**\n",
        "\n",
        "This trial I will use IterativeImputer and reduce the parameters for computational purposes. I assume IterativeImputer will provide a better score, through more accurate missing value estimates. Parameters changed are as follows:\n",
        "\n",
        "*   'my_classifier__n_estimators': [70, 80]\n",
        "*   'my_classifier__max_depth':[5, 10]"
      ],
      "id": "6qoLG-VvZXqM"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 708
        },
        "id": "1SSlsjXcORil",
        "outputId": "32d29753-44d8-4a40-dc6c-5b6d97014255"
      },
      "source": [
        "# Trial4\n",
        "\n",
        "# load modules and functions\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import f1_score\n",
        "from pprint import pprint\n",
        "\n",
        "# read in data\n",
        "\n",
        "data = pd.read_csv('train.csv')\n",
        "data_test = pd.read_csv('test.csv')\n",
        "# data.shape # return shape of the array, which is 5909 rows and 192 columns\n",
        "\n",
        "# data.isnull().sum().hist() # count of null values for columns as histogram\n",
        "# data['match'].hist() # count of 'match' column as histogram; binary, 1 is positive match\n",
        "\n",
        "#======================================================\n",
        "# down-grade scikit-learn (latest not greatest :)\n",
        "!pip install -Ivq scikit-learn==0.23.2\n",
        "\n",
        "# if you haven't installed xgboost on your system, uncomment the line below\n",
        "!pip install xgboost\n",
        "# if you haven't installed bayesian-optimization on your system, uncomment the line below\n",
        "!pip install scikit-optimize\n",
        "\n",
        "#======================================================\n",
        "x = data.drop('match', axis=1) # subset data as 'x' without 'match' feature\n",
        "features_numeric = list(x.select_dtypes(include=['float64'])) # subset all features with data type 'float64'\n",
        "features_categorical = list(x.select_dtypes(include=['object'])) # subset all features with data type 'object'\n",
        "y = data['match'] # the dependent feature or response variable 'match'\n",
        "\n",
        "# print(features_categorical) # print categorical features\n",
        "\n",
        "#====================================================\n",
        "\n",
        "# load modules and functions\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "# explicitly require this experimental feature\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "# now you can import normally from sklearn.impute\n",
        "from sklearn.impute import IterativeImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from xgboost.sklearn import XGBClassifier\n",
        "from sklearn.linear_model import BayesianRidge\n",
        "from sklearn.ensemble import ExtraTreesRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.impute import KNNImputer\n",
        "\n",
        "np.random.seed(0) # seed the number generator\n",
        "# define steps with name and estimator object for numeric feaeture transformation pipeline\n",
        "transformer_numeric = Pipeline(\n",
        "    steps=[\n",
        "        ('imputer', IterativeImputer()), ### imputation using IterativeImputer with default 10 imputation rounds and Bayesian Ridge estimator\n",
        "        ('scaler', StandardScaler())] # standardize features by removing mean and scaling to unit variance\n",
        ")\n",
        "# define pipeline steps for categorical feature transformation\n",
        "transformer_categorical = Pipeline(\n",
        "    steps=[\n",
        "        ('imputer', SimpleImputer(strategy='most_frequent')), # replace missing values with most frequent\n",
        "        ('onehot', OneHotEncoder(handle_unknown='ignore')) # columns will be all zeros if unknown categorical feature present during transform\n",
        "    ]\n",
        ")\n",
        "# specify transformers for preprocessing\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', transformer_numeric, features_numeric), # specify numeric transformer application to numeric features subset\n",
        "        ('cat', transformer_categorical, features_categorical) # specify categorical transformer application to categorical subset\n",
        "    ]\n",
        ")\n",
        "# specify full pipeline steps for preprocessing and estimator/classifier\n",
        "full_pipline = Pipeline(\n",
        "    steps=[\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('my_classifier',\n",
        "           # XGBClassifier(objective='binary:logistic', seed=1), (another example)\n",
        "           RandomForestClassifier(),\n",
        "        )\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "# `__` denotes attribute\n",
        "# (e.g. my_classifier__n_estimators means the `n_estimators` param for `my_classifier`\n",
        "#  which is our xgb)\n",
        "param_grid = {\n",
        "    # 'preprocessor__num__imputer__strategy': ['median','most_frequent','mean'], # this is for random forest\n",
        "    # 'my_classifier__n_estimators': [20, 30],  # this is for xgboost\n",
        "    # 'my_classifier__max_depth':[10, 20]       # this is for xgboost\n",
        "    'my_classifier__n_estimators': [70, 80],    # this is for random forest ======== best: 40, remove 20, 30 and add 50, 60\n",
        "    'my_classifier__max_depth':[5, 10]         # this is for random forest ======= best: 10, remove 30 and add 5\n",
        "}\n",
        "# instantiate GridSearchCV object with pipeline\n",
        "# cv=3: 3-fold cross validation for each combination of parameters\n",
        "# verbose=1: computation time for each fold and parameter candidate is displayed\n",
        "# n_jobs=2: run two jobs in parallel\n",
        "# use 'roc_auc' evaluation\n",
        "grid_search = GridSearchCV(\n",
        "    full_pipline, param_grid, cv=3, verbose=1, n_jobs=2,\n",
        "    scoring='roc_auc')\n",
        "\n",
        "grid_search.fit(x, y) # fit grid search to training data\n",
        "\n",
        "print('grid best score {}'.format(grid_search.best_score_)) # print best score\n",
        "print('grid best params {}'.format(grid_search.best_params_)) # print parameters associated with best score\n"
      ],
      "id": "1SSlsjXcORil",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikit-learn==0.23.2\n",
            "  Using cached scikit_learn-0.23.2-cp37-cp37m-manylinux1_x86_64.whl (6.8 MB)\n",
            "Collecting scipy>=0.19.1\n",
            "  Using cached scipy-1.7.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (28.5 MB)\n",
            "Collecting numpy>=1.13.3\n",
            "  Using cached numpy-1.21.3-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
            "Collecting threadpoolctl>=2.0.0\n",
            "  Using cached threadpoolctl-3.0.0-py3-none-any.whl (14 kB)\n",
            "Collecting joblib>=0.11\n",
            "  Using cached joblib-1.1.0-py2.py3-none-any.whl (306 kB)\n",
            "Installing collected packages: numpy, threadpoolctl, scipy, joblib, scikit-learn\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.6.0 requires numpy~=1.19.2, but you have numpy 1.21.3 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed joblib-1.1.0 numpy-1.21.3 scikit-learn-0.23.2 scipy-1.7.1 threadpoolctl-3.0.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "joblib",
                  "numpy",
                  "scipy",
                  "sklearn"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.7/dist-packages (0.90)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from xgboost) (1.7.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from xgboost) (1.21.3)\n",
            "Requirement already satisfied: scikit-optimize in /usr/local/lib/python3.7/dist-packages (0.9.0)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.7/dist-packages (from scikit-optimize) (0.23.2)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit-optimize) (1.7.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-optimize) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scikit-optimize) (1.21.3)\n",
            "Requirement already satisfied: pyaml>=16.9 in /usr/local/lib/python3.7/dist-packages (from scikit-optimize) (21.10.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from pyaml>=16.9->scikit-optimize) (3.13)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->scikit-optimize) (3.0.0)\n",
            "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  12 out of  12 | elapsed: 19.1min finished\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "grid best score 0.8449574113717282\n",
            "grid best params {'my_classifier__max_depth': 10, 'my_classifier__n_estimators': 70}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "ZO-GoWFIBB4_",
        "outputId": "d883df0c-2d45-4a63-c8d7-0a409418ad76"
      },
      "source": [
        "# create submission\n",
        "submission = pd.DataFrame() # create data frame\n",
        "submission['id'] = data_test['id'] # set 'id' column elements with those from 'id' in data_test\n",
        "submission['match'] = grid_search.predict_proba(data_test)[:,1] # probability of data belonging to class 1\n",
        "submission.to_csv('submission_trial4.csv', index=False) # save data to csv file without creating index\n",
        "submission # preview data"
      ],
      "id": "ZO-GoWFIBB4_",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>match</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>934</td>\n",
              "      <td>0.104986</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>6539</td>\n",
              "      <td>0.266696</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>6757</td>\n",
              "      <td>0.151237</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2275</td>\n",
              "      <td>0.075425</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1052</td>\n",
              "      <td>0.114601</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2464</th>\n",
              "      <td>7982</td>\n",
              "      <td>0.168967</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2465</th>\n",
              "      <td>7299</td>\n",
              "      <td>0.444485</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2466</th>\n",
              "      <td>1818</td>\n",
              "      <td>0.078560</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2467</th>\n",
              "      <td>937</td>\n",
              "      <td>0.091061</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2468</th>\n",
              "      <td>6691</td>\n",
              "      <td>0.157740</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2469 rows √ó 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        id     match\n",
              "0      934  0.104986\n",
              "1     6539  0.266696\n",
              "2     6757  0.151237\n",
              "3     2275  0.075425\n",
              "4     1052  0.114601\n",
              "...    ...       ...\n",
              "2464  7982  0.168967\n",
              "2465  7299  0.444485\n",
              "2466  1818  0.078560\n",
              "2467   937  0.091061\n",
              "2468  6691  0.157740\n",
              "\n",
              "[2469 rows x 2 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62rlh8mzAf-V"
      },
      "source": [
        "### **Trial5**\n",
        "\n",
        "Trial4 results:\n",
        "\n",
        "**grid best score 0.8429347495407877**\n",
        "\n",
        "**grid best params** {'my_classifier__max_depth': 5, 'my_classifier__n_estimators': 80}\n",
        "\n",
        "**submission scored 0.84902**\n",
        "\n",
        "The IterativeImputer was computationally expensive and the score is still similar to the SimpleImputer. I will continue using SimpleImputer.\n",
        "\n",
        "For Trial5 I will use random search for hyperparameter tuning. I predict that the score will be similar, but the time taken will be shorter (relative to previous SimpleImputer trials). This is because random search is not exhaustive. Parameters are as follows:\n",
        "\n",
        "\n",
        "\n",
        "*   'preprocessor__num__imputer__strategy': ['median','most_frequent', 'mean']\n",
        "*   'my_classifier__n_estimators': [60, 70, 80, 90]\n",
        "*   'my_classifier__max_depth':[1, 5, 10, 20]\n",
        "\n",
        "      "
      ],
      "id": "62rlh8mzAf-V"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 708
        },
        "id": "ed1I0LWAFSYt",
        "outputId": "c761d095-e88e-477a-ff2a-eb400022c550"
      },
      "source": [
        "# Trial5\n",
        "\n",
        "# load modules and functions\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import f1_score\n",
        "from pprint import pprint\n",
        "\n",
        "# read in data\n",
        "\n",
        "data = pd.read_csv('train.csv')\n",
        "data_test = pd.read_csv('test.csv')\n",
        "# data.shape # return shape of the array, which is 5909 rows and 192 columns\n",
        "\n",
        "# data.isnull().sum().hist() # count of null values for columns as histogram\n",
        "# data['match'].hist() # count of 'match' column as histogram; binary, 1 is positive match\n",
        "\n",
        "#======================================================\n",
        "# down-grade scikit-learn (latest not greatest :)\n",
        "!pip install -Ivq scikit-learn==0.23.2\n",
        "\n",
        "# if you haven't installed xgboost on your system, uncomment the line below\n",
        "!pip install xgboost\n",
        "# if you haven't installed bayesian-optimization on your system, uncomment the line below\n",
        "!pip install scikit-optimize\n",
        "\n",
        "#======================================================\n",
        "x = data.drop('match', axis=1) # subset data as 'x' without 'match' feature\n",
        "features_numeric = list(x.select_dtypes(include=['float64'])) # subset all features with data type 'float64'\n",
        "features_categorical = list(x.select_dtypes(include=['object'])) # subset all features with data type 'object'\n",
        "y = data['match'] # the dependent feature or response variable 'match'\n",
        "\n",
        "# print(features_categorical) # print categorical features\n",
        "\n",
        "#====================================================\n",
        "\n",
        "# load modules and functions\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "# explicitly require this experimental feature\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "# now you can import normally from sklearn.impute\n",
        "from sklearn.impute import IterativeImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from xgboost.sklearn import XGBClassifier\n",
        "from sklearn.linear_model import BayesianRidge\n",
        "from sklearn.ensemble import ExtraTreesRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "np.random.seed(0) # seed the number generator\n",
        "# define steps with name and estimator object for numeric feaeture transformation pipeline\n",
        "transformer_numeric = Pipeline(\n",
        "    steps=[\n",
        "        ('imputer', SimpleImputer(strategy='mean')), ### imputation using IterativeImputer with default 10 imputation rounds and Bayesian Ridge estimator\n",
        "        ('scaler', StandardScaler())] # standardize features by removing mean and scaling to unit variance\n",
        ")\n",
        "# define pipeline steps for categorical feature transformation\n",
        "transformer_categorical = Pipeline(\n",
        "    steps=[\n",
        "        ('imputer', SimpleImputer(strategy='most_frequent')), # replace missing values with most frequent\n",
        "        ('onehot', OneHotEncoder(handle_unknown='ignore')) # columns will be all zeros if unknown categorical feature present during transform\n",
        "    ]\n",
        ")\n",
        "# specify transformers for preprocessing\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', transformer_numeric, features_numeric), # specify numeric transformer application to numeric features subset\n",
        "        ('cat', transformer_categorical, features_categorical) # specify categorical transformer application to categorical subset\n",
        "    ]\n",
        ")\n",
        "# specify full pipeline steps for preprocessing and estimator/classifier\n",
        "full_pipline = Pipeline(\n",
        "    steps=[\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('my_classifier',\n",
        "           # XGBClassifier(objective='binary:logistic', seed=1), (another example)\n",
        "           RandomForestClassifier(),\n",
        "        )\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "# `__` denotes attribute\n",
        "# (e.g. my_classifier__n_estimators means the `n_estimators` param for `my_classifier`\n",
        "#  which is our xgb)\n",
        "param_grid = {\n",
        "    'preprocessor__num__imputer__strategy': ['median','most_frequent','mean'], # this is for random forest\n",
        "    # 'my_classifier__n_estimators': [20, 30],  # this is for xgboost\n",
        "    # 'my_classifier__max_depth':[10, 20]       # this is for xgboost\n",
        "    'my_classifier__n_estimators': [60, 70, 80, 90],    # this is for random forest ========\n",
        "    'my_classifier__max_depth':[1, 5, 10, 20]         # this is for random forest =======\n",
        "}\n",
        "#===================================================================\n",
        "# instantiate GridSearchCV object with pipeline\n",
        "# cv=3: 3-fold cross validation for each combination of parameters\n",
        "# verbose=1: computation time for each fold and parameter candidate is displayed\n",
        "# n_jobs=2: run two jobs in parallel\n",
        "# use 'roc_auc' evaluation\n",
        "\n",
        "# grid_search = GridSearchCV(\n",
        "#     full_pipline, param_grid, cv=3, verbose=1, n_jobs=2,\n",
        "#     scoring='roc_auc')\n",
        "\n",
        "# grid_search.fit(x, y) # fit grid search to training data\n",
        "\n",
        "random_search = RandomizedSearchCV(\n",
        "    full_pipline, param_grid, random_state=0, cv=3, verbose=1, n_jobs=2,\n",
        "    scoring='roc_auc')\n",
        "\n",
        "random_search.fit(x,y) # fit random_search to training data\n",
        "#===================================================================\n",
        "\n",
        "print('random best score {}'.format(random_search.best_score_)) # print best score\n",
        "print('random best params {}'.format(random_search.best_params_)) # print parameters associated with best score\n"
      ],
      "id": "ed1I0LWAFSYt",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikit-learn==0.23.2\n",
            "  Using cached scikit_learn-0.23.2-cp37-cp37m-manylinux1_x86_64.whl (6.8 MB)\n",
            "Collecting numpy>=1.13.3\n",
            "  Using cached numpy-1.21.3-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
            "Collecting scipy>=0.19.1\n",
            "  Using cached scipy-1.7.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (28.5 MB)\n",
            "Collecting threadpoolctl>=2.0.0\n",
            "  Using cached threadpoolctl-3.0.0-py3-none-any.whl (14 kB)\n",
            "Collecting joblib>=0.11\n",
            "  Using cached joblib-1.1.0-py2.py3-none-any.whl (306 kB)\n",
            "Installing collected packages: numpy, threadpoolctl, scipy, joblib, scikit-learn\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.6.0 requires numpy~=1.19.2, but you have numpy 1.21.3 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed joblib-1.1.0 numpy-1.21.3 scikit-learn-0.23.2 scipy-1.7.1 threadpoolctl-3.0.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "joblib",
                  "numpy",
                  "scipy",
                  "sklearn"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.7/dist-packages (0.90)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from xgboost) (1.21.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from xgboost) (1.7.1)\n",
            "Requirement already satisfied: scikit-optimize in /usr/local/lib/python3.7/dist-packages (0.9.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scikit-optimize) (1.21.3)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.7/dist-packages (from scikit-optimize) (0.23.2)\n",
            "Requirement already satisfied: pyaml>=16.9 in /usr/local/lib/python3.7/dist-packages (from scikit-optimize) (21.10.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-optimize) (1.1.0)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit-optimize) (1.7.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from pyaml>=16.9->scikit-optimize) (3.13)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->scikit-optimize) (3.0.0)\n",
            "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  30 out of  30 | elapsed:   20.4s finished\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "random best score 0.8445544249742766\n",
            "random best params {'preprocessor__num__imputer__strategy': 'mean', 'my_classifier__n_estimators': 80, 'my_classifier__max_depth': 10}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-uJmUlmkggI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "0dcc3adf-4384-43b9-949d-687a6e5a96b5"
      },
      "source": [
        "# create submission\n",
        "submission = pd.DataFrame() # create data frame\n",
        "submission['id'] = data_test['id'] # set 'id' column elements with those from 'id' in data_test\n",
        "submission['match'] = random_search.predict_proba(data_test)[:,1] # probability of data belonging to class 1\n",
        "submission.to_csv('submission_trial5.csv', index=False) # save data to csv file without creating index\n",
        "submission # preview data"
      ],
      "id": "5-uJmUlmkggI",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>match</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>934</td>\n",
              "      <td>0.089907</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>6539</td>\n",
              "      <td>0.217972</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>6757</td>\n",
              "      <td>0.198676</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2275</td>\n",
              "      <td>0.102680</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1052</td>\n",
              "      <td>0.123625</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2464</th>\n",
              "      <td>7982</td>\n",
              "      <td>0.192284</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2465</th>\n",
              "      <td>7299</td>\n",
              "      <td>0.380262</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2466</th>\n",
              "      <td>1818</td>\n",
              "      <td>0.101737</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2467</th>\n",
              "      <td>937</td>\n",
              "      <td>0.144713</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2468</th>\n",
              "      <td>6691</td>\n",
              "      <td>0.092936</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2469 rows √ó 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        id     match\n",
              "0      934  0.089907\n",
              "1     6539  0.217972\n",
              "2     6757  0.198676\n",
              "3     2275  0.102680\n",
              "4     1052  0.123625\n",
              "...    ...       ...\n",
              "2464  7982  0.192284\n",
              "2465  7299  0.380262\n",
              "2466  1818  0.101737\n",
              "2467   937  0.144713\n",
              "2468  6691  0.092936\n",
              "\n",
              "[2469 rows x 2 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PANjq91PPJpj"
      },
      "source": [
        "### **Trial6**\n",
        "\n",
        "Trial5 results below is similar to the grid search score, but the computation time is much shorter.\n",
        "\n",
        "**random best score 0.8444805388892612**\n",
        "\n",
        "**random best params** {'preprocessor__num__imputer__strategy': 'most_frequent', 'my_classifier__n_estimators': 90, 'my_classifier__max_depth': 10}\n",
        "\n",
        "**submission scored 0.85309**\n",
        "\n",
        "For this trial I will try a bayesian search. I predict that the results will be similar, but the computation will be quicker due to the learning of the bayesian model. It is not exhaustive in its parameter search like grid search."
      ],
      "id": "PANjq91PPJpj"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 871
        },
        "id": "FM3f7pa4XyZy",
        "outputId": "32423ac2-5232-46b9-c7ad-95410a06c6b6"
      },
      "source": [
        "# Trial6\n",
        "\n",
        "# load modules and functions\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import f1_score\n",
        "from pprint import pprint\n",
        "\n",
        "# read in data\n",
        "\n",
        "data = pd.read_csv('train.csv')\n",
        "data_test = pd.read_csv('test.csv')\n",
        "# data.shape # return shape of the array, which is 5909 rows and 192 columns\n",
        "\n",
        "# data.isnull().sum().hist() # count of null values for columns as histogram\n",
        "# data['match'].hist() # count of 'match' column as histogram; binary, 1 is positive match\n",
        "\n",
        "#======================================================\n",
        "# down-grade scikit-learn (latest not greatest :)\n",
        "!pip install -Ivq scikit-learn==0.23.2\n",
        "\n",
        "# if you haven't installed xgboost on your system, uncomment the line below\n",
        "!pip install xgboost\n",
        "# if you haven't installed bayesian-optimization on your system, uncomment the line below\n",
        "!pip install scikit-optimize\n",
        "\n",
        "#======================================================\n",
        "x = data.drop('match', axis=1) # subset data as 'x' without 'match' feature\n",
        "features_numeric = list(x.select_dtypes(include=['float64'])) # subset all features with data type 'float64'\n",
        "features_categorical = list(x.select_dtypes(include=['object'])) # subset all features with data type 'object'\n",
        "y = data['match'] # the dependent feature or response variable 'match'\n",
        "\n",
        "# print(features_categorical) # print categorical features\n",
        "\n",
        "#====================================================\n",
        "\n",
        "# load modules and functions\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "# explicitly require this experimental feature\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "# now you can import normally from sklearn.impute\n",
        "from sklearn.impute import IterativeImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from xgboost.sklearn import XGBClassifier\n",
        "from sklearn.linear_model import BayesianRidge\n",
        "from sklearn.ensemble import ExtraTreesRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "np.random.seed(0) # seed the number generator\n",
        "# define steps with name and estimator object for numeric feaeture transformation pipeline\n",
        "transformer_numeric = Pipeline(\n",
        "    steps=[\n",
        "        ('imputer', SimpleImputer(strategy='mean')), ### imputation using IterativeImputer with default 10 imputation rounds and Bayesian Ridge estimator\n",
        "        ('scaler', StandardScaler())] # standardize features by removing mean and scaling to unit variance\n",
        ")\n",
        "# define pipeline steps for categorical feature transformation\n",
        "transformer_categorical = Pipeline(\n",
        "    steps=[\n",
        "        ('imputer', SimpleImputer(strategy='most_frequent')), # replace missing values with most frequent\n",
        "        ('onehot', OneHotEncoder(handle_unknown='ignore')) # columns will be all zeros if unknown categorical feature present during transform\n",
        "    ]\n",
        ")\n",
        "# specify transformers for preprocessing\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', transformer_numeric, features_numeric), # specify numeric transformer application to numeric features subset\n",
        "        ('cat', transformer_categorical, features_categorical) # specify categorical transformer application to categorical subset\n",
        "    ]\n",
        ")\n",
        "# specify full pipeline steps for preprocessing and estimator/classifier\n",
        "full_pipline = Pipeline(\n",
        "    steps=[\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('my_classifier',\n",
        "           # XGBClassifier(objective='binary:logistic', seed=1), (another example)\n",
        "           RandomForestClassifier(),\n",
        "        )\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "# import classes\n",
        "\n",
        "from skopt import BayesSearchCV\n",
        "from skopt.space import Real, Categorical, Integer\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "\n",
        "# define bayesian optimization estimator and search spaces\n",
        "# search spaces contained in dictionary, where keys are parameter names and values are skopt.space.Dimension instances\n",
        "\n",
        "bayes_search = BayesSearchCV(\n",
        "    full_pipline,\n",
        "    {\n",
        "        'preprocessor__num__imputer__strategy': Categorical(['median','most_frequent','mean']),\n",
        "        'my_classifier__n_estimators': Integer(1, 200, prior='log-uniform'),\n",
        "        'my_classifier__max_depth': Integer(1, 1000, prior='log-uniform'),\n",
        "    },\n",
        "    n_iter=4, # number of parameter settings that are sampled\n",
        "    random_state=0, # use random numbers generated for random_state=0\n",
        "    verbose=1, # set verbosity to 1 for level of details of CV\n",
        "    cv=3, # 3-fold cross validation for each combination of parameters,\n",
        "    scoring='roc_auc'\n",
        ")\n",
        "\n",
        "bayes_search.fit(x, y) # execute bayesian optimization\n",
        "\n",
        "print('best score {}'.format(bayes_search.best_score_)) # print best score from bayesian optimization\n",
        "print('best params {}'.format(bayes_search.best_params_)) # print parameters associated with best score"
      ],
      "id": "FM3f7pa4XyZy",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikit-learn==0.23.2\n",
            "  Using cached scikit_learn-0.23.2-cp37-cp37m-manylinux1_x86_64.whl (6.8 MB)\n",
            "Collecting scipy>=0.19.1\n",
            "  Using cached scipy-1.7.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (28.5 MB)\n",
            "Collecting threadpoolctl>=2.0.0\n",
            "  Using cached threadpoolctl-3.0.0-py3-none-any.whl (14 kB)\n",
            "Collecting numpy>=1.13.3\n",
            "  Using cached numpy-1.21.3-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
            "Collecting joblib>=0.11\n",
            "  Using cached joblib-1.1.0-py2.py3-none-any.whl (306 kB)\n",
            "Installing collected packages: numpy, threadpoolctl, scipy, joblib, scikit-learn\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.6.0 requires numpy~=1.19.2, but you have numpy 1.21.3 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed joblib-1.1.0 numpy-1.21.3 scikit-learn-0.23.2 scipy-1.7.1 threadpoolctl-3.0.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "joblib",
                  "numpy",
                  "scipy",
                  "sklearn"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.7/dist-packages (0.90)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from xgboost) (1.7.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from xgboost) (1.21.3)\n",
            "Requirement already satisfied: scikit-optimize in /usr/local/lib/python3.7/dist-packages (0.9.0)\n",
            "Requirement already satisfied: pyaml>=16.9 in /usr/local/lib/python3.7/dist-packages (from scikit-optimize) (21.10.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.7/dist-packages (from scikit-optimize) (0.23.2)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit-optimize) (1.7.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scikit-optimize) (1.21.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-optimize) (1.1.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from pyaml>=16.9->scikit-optimize) (3.13)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->scikit-optimize) (3.0.0)\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    3.8s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    3.7s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.7s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    1.3s finished\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "best score 0.8453218712389318\n",
            "best params OrderedDict([('my_classifier__max_depth', 6), ('my_classifier__n_estimators', 179), ('preprocessor__num__imputer__strategy', 'most_frequent')])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "0CWaW742D-wI",
        "outputId": "db1505a5-9243-49fb-f750-1e5f07a1e3e8"
      },
      "source": [
        "# create submission\n",
        "submission = pd.DataFrame() # create data frame\n",
        "submission['id'] = data_test['id'] # set 'id' column elements with those from 'id' in data_test\n",
        "submission['match'] = bayes_search.predict_proba(data_test)[:,1] # probability of data belonging to class 1\n",
        "submission.to_csv('submission_trial6.csv', index=False) # save data to csv file without creating index\n",
        "submission # preview data"
      ],
      "id": "0CWaW742D-wI",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>match</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>934</td>\n",
              "      <td>0.126683</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>6539</td>\n",
              "      <td>0.224648</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>6757</td>\n",
              "      <td>0.186002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2275</td>\n",
              "      <td>0.123105</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1052</td>\n",
              "      <td>0.126481</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2464</th>\n",
              "      <td>7982</td>\n",
              "      <td>0.164443</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2465</th>\n",
              "      <td>7299</td>\n",
              "      <td>0.257778</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2466</th>\n",
              "      <td>1818</td>\n",
              "      <td>0.113575</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2467</th>\n",
              "      <td>937</td>\n",
              "      <td>0.143189</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2468</th>\n",
              "      <td>6691</td>\n",
              "      <td>0.125540</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2469 rows √ó 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        id     match\n",
              "0      934  0.126683\n",
              "1     6539  0.224648\n",
              "2     6757  0.186002\n",
              "3     2275  0.123105\n",
              "4     1052  0.126481\n",
              "...    ...       ...\n",
              "2464  7982  0.164443\n",
              "2465  7299  0.257778\n",
              "2466  1818  0.113575\n",
              "2467   937  0.143189\n",
              "2468  6691  0.125540\n",
              "\n",
              "[2469 rows x 2 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSYraICNmmUz"
      },
      "source": [
        "### **Trial7**\n",
        "\n",
        "Bayesian optimization seems to be the quickest with the best score so far, as shown below.\n",
        "\n",
        "**best score 0.8453218712389318**\n",
        "\n",
        "**best params** :([('my_classifier__max_depth', 6), ('my_classifier__n_estimators', 179), ('preprocessor__num__imputer__strategy', 'most_frequent')])\n",
        "\n",
        "**submission scored 0.84688**\n",
        "\n",
        "I will use a boosting model, XGBoostClassifier next to compare it with the RandomForestClassifier (a bagging model).Since boosting models use iterative learning, I predict that the XGBoostClassifier will provide a better score than the RandomForestClassifier. The following will be the search spaces.\n",
        "\n",
        "\n",
        "\n",
        "*   'my_classifier__n_estimators': Integer(1, 300, prior='log-uniform')\n",
        "*   'my_classifier__max_depth':Integer(1, 100, prior='log-uniform')\n"
      ],
      "id": "iSYraICNmmUz"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 871
        },
        "id": "zdVko5V2hNW8",
        "outputId": "b3873871-fbbf-453f-c13b-d23137c6a59d"
      },
      "source": [
        "# Trial7\n",
        "\n",
        "# load modules and functions\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import f1_score\n",
        "from pprint import pprint\n",
        "\n",
        "# read in data\n",
        "\n",
        "data = pd.read_csv('train.csv')\n",
        "data_test = pd.read_csv('test.csv')\n",
        "# data.shape # return shape of the array, which is 5909 rows and 192 columns\n",
        "\n",
        "# data.isnull().sum().hist() # count of null values for columns as histogram\n",
        "# data['match'].hist() # count of 'match' column as histogram; binary, 1 is positive match\n",
        "\n",
        "#======================================================\n",
        "# down-grade scikit-learn (latest not greatest :)\n",
        "!pip install -Ivq scikit-learn==0.23.2\n",
        "\n",
        "# if you haven't installed xgboost on your system, uncomment the line below\n",
        "!pip install xgboost\n",
        "# if you haven't installed bayesian-optimization on your system, uncomment the line below\n",
        "!pip install scikit-optimize\n",
        "\n",
        "#======================================================\n",
        "x = data.drop('match', axis=1) # subset data as 'x' without 'match' feature\n",
        "features_numeric = list(x.select_dtypes(include=['float64'])) # subset all features with data type 'float64'\n",
        "features_categorical = list(x.select_dtypes(include=['object'])) # subset all features with data type 'object'\n",
        "y = data['match'] # the dependent feature or response variable 'match'\n",
        "\n",
        "# print(features_categorical) # print categorical features\n",
        "\n",
        "#====================================================\n",
        "\n",
        "# load modules and functions\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "# explicitly require this experimental feature\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "# now you can import normally from sklearn.impute\n",
        "from sklearn.impute import IterativeImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from xgboost.sklearn import XGBClassifier\n",
        "from sklearn.linear_model import BayesianRidge\n",
        "from sklearn.ensemble import ExtraTreesRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "np.random.seed(0) # seed the number generator\n",
        "# define steps with name and estimator object for numeric feaeture transformation pipeline\n",
        "transformer_numeric = Pipeline(\n",
        "    steps=[\n",
        "        ('imputer', SimpleImputer(strategy='mean')), ### imputation using IterativeImputer with default 10 imputation rounds and Bayesian Ridge estimator\n",
        "        ('scaler', StandardScaler())] # standardize features by removing mean and scaling to unit variance\n",
        ")\n",
        "# define pipeline steps for categorical feature transformation\n",
        "transformer_categorical = Pipeline(\n",
        "    steps=[\n",
        "        ('imputer', SimpleImputer(strategy='most_frequent')), # replace missing values with most frequent\n",
        "        ('onehot', OneHotEncoder(handle_unknown='ignore')) # columns will be all zeros if unknown categorical feature present during transform\n",
        "    ]\n",
        ")\n",
        "# specify transformers for preprocessing\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', transformer_numeric, features_numeric), # specify numeric transformer application to numeric features subset\n",
        "        ('cat', transformer_categorical, features_categorical) # specify categorical transformer application to categorical subset\n",
        "    ]\n",
        ")\n",
        "# specify full pipeline steps for preprocessing and estimator/classifier\n",
        "full_pipline = Pipeline(\n",
        "    steps=[\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('my_classifier',\n",
        "           XGBClassifier(objective='binary:logistic', seed=1), # (another example)\n",
        "          #  RandomForestClassifier(),\n",
        "        )\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "# import classes\n",
        "\n",
        "from skopt import BayesSearchCV\n",
        "from skopt.space import Real, Categorical, Integer\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "\n",
        "# define bayesian optimization estimator and search spaces\n",
        "# search spaces contained in dictionary, where keys are parameter names and values are skopt.space.Dimension instances\n",
        "\n",
        "bayes_search = BayesSearchCV(\n",
        "    full_pipline,\n",
        "    {\n",
        "        # 'preprocessor__num__imputer__strategy': Categorical(['median','most_frequent','mean']), # this is for random forest\n",
        "        'my_classifier__n_estimators': Integer(1, 300, prior='log-uniform'),  # this is for xgboost\n",
        "        'my_classifier__max_depth':Integer(1, 100, prior='log-uniform')       # this is for xgboost\n",
        "        # 'my_classifier__n_estimators': Integer(1, 200, prior='log-uniform'), # for random forest\n",
        "        # 'my_classifier__max_depth': Integer(1, 1000, prior='log-uniform'), # for random forest\n",
        "    },\n",
        "    n_iter=4, # number of parameter settings that are sampled\n",
        "    random_state=0, # use random numbers generated for random_state=0\n",
        "    verbose=1, # set verbosity to 1 for level of details of CV\n",
        "    cv=3, # 3-fold cross validation for each combination of parameters,\n",
        "    scoring='roc_auc'\n",
        ")\n",
        "\n",
        "bayes_search.fit(x, y) # execute bayesian optimization\n",
        "\n",
        "print('bayesXG best score {}'.format(bayes_search.best_score_)) # print best score from bayesian optimization\n",
        "print('bayesXG best params {}'.format(bayes_search.best_params_)) # print parameters associated with best score"
      ],
      "id": "zdVko5V2hNW8",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikit-learn==0.23.2\n",
            "  Using cached scikit_learn-0.23.2-cp37-cp37m-manylinux1_x86_64.whl (6.8 MB)\n",
            "Collecting threadpoolctl>=2.0.0\n",
            "  Using cached threadpoolctl-3.0.0-py3-none-any.whl (14 kB)\n",
            "Collecting scipy>=0.19.1\n",
            "  Using cached scipy-1.7.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (28.5 MB)\n",
            "Collecting numpy>=1.13.3\n",
            "  Using cached numpy-1.21.3-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
            "Collecting joblib>=0.11\n",
            "  Using cached joblib-1.1.0-py2.py3-none-any.whl (306 kB)\n",
            "Installing collected packages: numpy, threadpoolctl, scipy, joblib, scikit-learn\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.6.0 requires numpy~=1.19.2, but you have numpy 1.21.3 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed joblib-1.1.0 numpy-1.21.3 scikit-learn-0.23.2 scipy-1.7.1 threadpoolctl-3.0.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "joblib",
                  "numpy",
                  "scipy",
                  "sklearn"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.7/dist-packages (0.90)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from xgboost) (1.7.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from xgboost) (1.21.3)\n",
            "Requirement already satisfied: scikit-optimize in /usr/local/lib/python3.7/dist-packages (0.9.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scikit-optimize) (1.21.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-optimize) (1.1.0)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit-optimize) (1.7.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.7/dist-packages (from scikit-optimize) (0.23.2)\n",
            "Requirement already satisfied: pyaml>=16.9 in /usr/local/lib/python3.7/dist-packages (from scikit-optimize) (21.10.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from pyaml>=16.9->scikit-optimize) (3.13)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->scikit-optimize) (3.0.0)\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:   13.2s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:   14.6s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    1.5s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    4.3s finished\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bayesXG best score 0.8779973567135082\n",
            "bayesXG best params OrderedDict([('my_classifier__max_depth', 3), ('my_classifier__n_estimators', 266)])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "BXTnhYA6IuC9",
        "outputId": "8419137f-a07d-4c49-c0f3-002649174997"
      },
      "source": [
        "# create submission\n",
        "submission = pd.DataFrame() # create data frame\n",
        "submission['id'] = data_test['id'] # set 'id' column elements with those from 'id' in data_test\n",
        "submission['match'] = bayes_search.predict_proba(data_test)[:,1] # probability of data belonging to class 1\n",
        "submission.to_csv('submission_trial7.csv', index=False) # save data to csv file without creating index\n",
        "submission # preview data"
      ],
      "id": "BXTnhYA6IuC9",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>match</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>934</td>\n",
              "      <td>0.060695</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>6539</td>\n",
              "      <td>0.474128</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>6757</td>\n",
              "      <td>0.250124</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2275</td>\n",
              "      <td>0.043435</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1052</td>\n",
              "      <td>0.017844</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2464</th>\n",
              "      <td>7982</td>\n",
              "      <td>0.160607</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2465</th>\n",
              "      <td>7299</td>\n",
              "      <td>0.508416</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2466</th>\n",
              "      <td>1818</td>\n",
              "      <td>0.066026</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2467</th>\n",
              "      <td>937</td>\n",
              "      <td>0.031247</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2468</th>\n",
              "      <td>6691</td>\n",
              "      <td>0.015211</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2469 rows √ó 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        id     match\n",
              "0      934  0.060695\n",
              "1     6539  0.474128\n",
              "2     6757  0.250124\n",
              "3     2275  0.043435\n",
              "4     1052  0.017844\n",
              "...    ...       ...\n",
              "2464  7982  0.160607\n",
              "2465  7299  0.508416\n",
              "2466  1818  0.066026\n",
              "2467   937  0.031247\n",
              "2468  6691  0.015211\n",
              "\n",
              "[2469 rows x 2 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7LEkkbxC0Nzj"
      },
      "source": [
        "### **Trial8**\n",
        "\n",
        "The score has improved, as shown below. I will attempt to address the class imbalance by setting the scale_pos_weight. According to the [documentation](https://xgboost.readthedocs.io/en/latest/parameter.html), the parameter should be set to the sum of negative (no match) / sum of positive (match). I check using data['match'].value_counts() below and determine that the scale_pos_weight will be set to 4921/988.\n",
        "\n",
        "\n",
        "**bayesXG best score 0.8779973567135082**\n",
        "\n",
        "**bayesXG best params** : ([('my_classifier__max_depth', 3), ('my_classifier__n_estimators', 266)])\n",
        "\n",
        "**submission scored 0.88173**"
      ],
      "id": "7LEkkbxC0Nzj"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s53u_-esxuO2",
        "outputId": "9f9e063a-26f0-4df4-8036-b91ef8948917"
      },
      "source": [
        "data['match'].value_counts()"
      ],
      "id": "s53u_-esxuO2",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    4921\n",
              "1     988\n",
              "Name: match, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 871
        },
        "id": "dmDGLfj09mlY",
        "outputId": "1064ddce-f2b8-4553-b641-0da6b2f10182"
      },
      "source": [
        "# Trial8\n",
        "\n",
        "# load modules and functions\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import f1_score\n",
        "from pprint import pprint\n",
        "\n",
        "# read in data\n",
        "\n",
        "data = pd.read_csv('train.csv')\n",
        "data_test = pd.read_csv('test.csv')\n",
        "# data.shape # return shape of the array, which is 5909 rows and 192 columns\n",
        "\n",
        "# data.isnull().sum().hist() # count of null values for columns as histogram\n",
        "# data['match'].hist() # count of 'match' column as histogram; binary, 1 is positive match\n",
        "\n",
        "#======================================================\n",
        "# down-grade scikit-learn (latest not greatest :)\n",
        "!pip install -Ivq scikit-learn==0.23.2\n",
        "\n",
        "# if you haven't installed xgboost on your system, uncomment the line below\n",
        "!pip install xgboost\n",
        "# if you haven't installed bayesian-optimization on your system, uncomment the line below\n",
        "!pip install scikit-optimize\n",
        "\n",
        "#======================================================\n",
        "x = data.drop('match', axis=1) # subset data as 'x' without 'match' feature\n",
        "features_numeric = list(x.select_dtypes(include=['float64'])) # subset all features with data type 'float64'\n",
        "features_categorical = list(x.select_dtypes(include=['object'])) # subset all features with data type 'object'\n",
        "y = data['match'] # the dependent feature or response variable 'match'\n",
        "\n",
        "# print(features_categorical) # print categorical features\n",
        "\n",
        "#====================================================\n",
        "\n",
        "# load modules and functions\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "# explicitly require this experimental feature\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "# now you can import normally from sklearn.impute\n",
        "from sklearn.impute import IterativeImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from xgboost.sklearn import XGBClassifier\n",
        "from sklearn.linear_model import BayesianRidge\n",
        "from sklearn.ensemble import ExtraTreesRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "np.random.seed(0) # seed the number generator\n",
        "# define steps with name and estimator object for numeric feaeture transformation pipeline\n",
        "transformer_numeric = Pipeline(\n",
        "    steps=[\n",
        "        ('imputer', SimpleImputer(strategy='mean')), ### imputation using IterativeImputer with default 10 imputation rounds and Bayesian Ridge estimator\n",
        "        ('scaler', StandardScaler())] # standardize features by removing mean and scaling to unit variance\n",
        ")\n",
        "# define pipeline steps for categorical feature transformation\n",
        "transformer_categorical = Pipeline(\n",
        "    steps=[\n",
        "        ('imputer', SimpleImputer(strategy='most_frequent')), # replace missing values with most frequent\n",
        "        ('onehot', OneHotEncoder(handle_unknown='ignore')) # columns will be all zeros if unknown categorical feature present during transform\n",
        "    ]\n",
        ")\n",
        "# specify transformers for preprocessing\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', transformer_numeric, features_numeric), # specify numeric transformer application to numeric features subset\n",
        "        ('cat', transformer_categorical, features_categorical) # specify categorical transformer application to categorical subset\n",
        "    ]\n",
        ")\n",
        "# specify full pipeline steps for preprocessing and estimator/classifier\n",
        "scale_pos_value = (4921/988) # value for scale_pos_weight\n",
        "full_pipline = Pipeline(\n",
        "    steps=[\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('my_classifier',\n",
        "           XGBClassifier(objective='binary:logistic', scale_pos_weight=scale_pos_value, seed=1), # set scale_pos_weight to sum(negative class)/sum(positive class)\n",
        "          #  RandomForestClassifier(),\n",
        "        )\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "# import classes\n",
        "\n",
        "from skopt import BayesSearchCV\n",
        "from skopt.space import Real, Categorical, Integer\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "\n",
        "# define bayesian optimization estimator and search spaces\n",
        "# search spaces contained in dictionary, where keys are parameter names and values are skopt.space.Dimension instances\n",
        "\n",
        "bayes_search = BayesSearchCV(\n",
        "    full_pipline,\n",
        "    {\n",
        "        # 'preprocessor__num__imputer__strategy': Categorical(['median','most_frequent','mean']), # this is for random forest\n",
        "        'my_classifier__n_estimators': Integer(1, 300, prior='log-uniform'),  # this is for xgboost\n",
        "        'my_classifier__max_depth':Integer(1, 100, prior='log-uniform')       # this is for xgboost\n",
        "        # 'my_classifier__n_estimators': Integer(1, 200, prior='log-uniform'), # for random forest\n",
        "        # 'my_classifier__max_depth': Integer(1, 1000, prior='log-uniform'), # for random forest\n",
        "    },\n",
        "    n_iter=4, # number of parameter settings that are sampled\n",
        "    random_state=0, # use random numbers generated for random_state=0\n",
        "    verbose=1, # set verbosity to 1 for level of details of CV\n",
        "    cv=3, # 3-fold cross validation for each combination of parameters,\n",
        "    scoring='roc_auc'\n",
        ")\n",
        "\n",
        "bayes_search.fit(x, y) # execute bayesian optimization\n",
        "\n",
        "print('bayesXG best score {}'.format(bayes_search.best_score_)) # print best score from bayesian optimization\n",
        "print('bayesXG best params {}'.format(bayes_search.best_params_)) # print parameters associated with best score"
      ],
      "id": "dmDGLfj09mlY",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikit-learn==0.23.2\n",
            "  Using cached scikit_learn-0.23.2-cp37-cp37m-manylinux1_x86_64.whl (6.8 MB)\n",
            "Collecting joblib>=0.11\n",
            "  Using cached joblib-1.1.0-py2.py3-none-any.whl (306 kB)\n",
            "Collecting numpy>=1.13.3\n",
            "  Using cached numpy-1.21.3-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
            "Collecting scipy>=0.19.1\n",
            "  Using cached scipy-1.7.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (28.5 MB)\n",
            "Collecting threadpoolctl>=2.0.0\n",
            "  Using cached threadpoolctl-3.0.0-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: numpy, threadpoolctl, scipy, joblib, scikit-learn\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.6.0 requires numpy~=1.19.2, but you have numpy 1.21.3 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed joblib-1.1.0 numpy-1.21.3 scikit-learn-0.23.2 scipy-1.7.1 threadpoolctl-3.0.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "joblib",
                  "numpy",
                  "scipy",
                  "sklearn"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.7/dist-packages (0.90)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from xgboost) (1.7.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from xgboost) (1.21.3)\n",
            "Requirement already satisfied: scikit-optimize in /usr/local/lib/python3.7/dist-packages (0.9.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-optimize) (1.1.0)\n",
            "Requirement already satisfied: pyaml>=16.9 in /usr/local/lib/python3.7/dist-packages (from scikit-optimize) (21.10.1)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit-optimize) (1.7.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scikit-optimize) (1.21.3)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.7/dist-packages (from scikit-optimize) (0.23.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from pyaml>=16.9->scikit-optimize) (3.13)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->scikit-optimize) (3.0.0)\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:   13.7s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:   14.6s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    1.5s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    4.3s finished\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bayesXG best score 0.875868092302026\n",
            "bayesXG best params OrderedDict([('my_classifier__max_depth', 3), ('my_classifier__n_estimators', 266)])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "QHIfHmVSJzZu",
        "outputId": "97eda214-1656-4671-9525-cca7158945a1"
      },
      "source": [
        "# create submission\n",
        "submission = pd.DataFrame() # create data frame\n",
        "submission['id'] = data_test['id'] # set 'id' column elements with those from 'id' in data_test\n",
        "submission['match'] = bayes_search.predict_proba(data_test)[:,1] # probability of data belonging to class 1\n",
        "submission.to_csv('submission_trial8.csv', index=False) # save data to csv file without creating index\n",
        "submission # preview data"
      ],
      "id": "QHIfHmVSJzZu",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>match</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>934</td>\n",
              "      <td>0.139085</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>6539</td>\n",
              "      <td>0.688074</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>6757</td>\n",
              "      <td>0.617107</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2275</td>\n",
              "      <td>0.221617</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1052</td>\n",
              "      <td>0.047273</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2464</th>\n",
              "      <td>7982</td>\n",
              "      <td>0.594791</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2465</th>\n",
              "      <td>7299</td>\n",
              "      <td>0.835148</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2466</th>\n",
              "      <td>1818</td>\n",
              "      <td>0.169870</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2467</th>\n",
              "      <td>937</td>\n",
              "      <td>0.069298</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2468</th>\n",
              "      <td>6691</td>\n",
              "      <td>0.053805</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2469 rows √ó 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        id     match\n",
              "0      934  0.139085\n",
              "1     6539  0.688074\n",
              "2     6757  0.617107\n",
              "3     2275  0.221617\n",
              "4     1052  0.047273\n",
              "...    ...       ...\n",
              "2464  7982  0.594791\n",
              "2465  7299  0.835148\n",
              "2466  1818  0.169870\n",
              "2467   937  0.069298\n",
              "2468  6691  0.053805\n",
              "\n",
              "[2469 rows x 2 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_14GgBwBacL"
      },
      "source": [
        "### **Trial9**\n",
        "\n",
        "The submission score improved slightly so I will keep the scale_pos_weight setting.\n",
        "\n",
        "**bayesXG best score 0.875868092302026**\n",
        "\n",
        "**bayesXG best params** : ([('my_classifier__max_depth', 3), ('my_classifier__n_estimators', 266)])\n",
        "\n",
        "**submission scored 0.88672**\n",
        "\n",
        "I will expand on the search spaces to see if the model can be further optimized."
      ],
      "id": "4_14GgBwBacL"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 871
        },
        "id": "U0ITlsrIIBTq",
        "outputId": "52e751ec-e98b-43ba-f414-64c7f9e4ee5e"
      },
      "source": [
        "# Trial9\n",
        "\n",
        "# load modules and functions\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import f1_score\n",
        "from pprint import pprint\n",
        "\n",
        "# read in data\n",
        "\n",
        "data = pd.read_csv('train.csv')\n",
        "data_test = pd.read_csv('test.csv')\n",
        "# data.shape # return shape of the array, which is 5909 rows and 192 columns\n",
        "\n",
        "# data.isnull().sum().hist() # count of null values for columns as histogram\n",
        "# data['match'].hist() # count of 'match' column as histogram; binary, 1 is positive match\n",
        "\n",
        "#======================================================\n",
        "# down-grade scikit-learn (latest not greatest :)\n",
        "!pip install -Ivq scikit-learn==0.23.2\n",
        "\n",
        "# if you haven't installed xgboost on your system, uncomment the line below\n",
        "!pip install xgboost\n",
        "# if you haven't installed bayesian-optimization on your system, uncomment the line below\n",
        "!pip install scikit-optimize\n",
        "\n",
        "#======================================================\n",
        "x = data.drop('match', axis=1) # subset data as 'x' without 'match' feature\n",
        "features_numeric = list(x.select_dtypes(include=['float64'])) # subset all features with data type 'float64'\n",
        "features_categorical = list(x.select_dtypes(include=['object'])) # subset all features with data type 'object'\n",
        "y = data['match'] # the dependent feature or response variable 'match'\n",
        "\n",
        "# print(features_categorical) # print categorical features\n",
        "\n",
        "#====================================================\n",
        "\n",
        "# load modules and functions\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "# explicitly require this experimental feature\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "# now you can import normally from sklearn.impute\n",
        "from sklearn.impute import IterativeImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from xgboost.sklearn import XGBClassifier\n",
        "from sklearn.linear_model import BayesianRidge\n",
        "from sklearn.ensemble import ExtraTreesRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "np.random.seed(0) # seed the number generator\n",
        "# define steps with name and estimator object for numeric feaeture transformation pipeline\n",
        "transformer_numeric = Pipeline(\n",
        "    steps=[\n",
        "        ('imputer', SimpleImputer(strategy='mean')), ### imputation using IterativeImputer with default 10 imputation rounds and Bayesian Ridge estimator\n",
        "        ('scaler', StandardScaler())] # standardize features by removing mean and scaling to unit variance\n",
        ")\n",
        "# define pipeline steps for categorical feature transformation\n",
        "transformer_categorical = Pipeline(\n",
        "    steps=[\n",
        "        ('imputer', SimpleImputer(strategy='most_frequent')), # replace missing values with most frequent\n",
        "        ('onehot', OneHotEncoder(handle_unknown='ignore')) # columns will be all zeros if unknown categorical feature present during transform\n",
        "    ]\n",
        ")\n",
        "# specify transformers for preprocessing\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', transformer_numeric, features_numeric), # specify numeric transformer application to numeric features subset\n",
        "        ('cat', transformer_categorical, features_categorical) # specify categorical transformer application to categorical subset\n",
        "    ]\n",
        ")\n",
        "# specify full pipeline steps for preprocessing and estimator/classifier\n",
        "scale_pos_value = (4921/988) # value for scale_pos_weight\n",
        "full_pipline = Pipeline(\n",
        "    steps=[\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('my_classifier',\n",
        "           XGBClassifier(objective='binary:logistic', scale_pos_weight=scale_pos_value, seed=1), # set scale_pos_weight to sum(negative class)/sum(positive class)\n",
        "          #  RandomForestClassifier(),\n",
        "        )\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "# import classes\n",
        "\n",
        "from skopt import BayesSearchCV\n",
        "from skopt.space import Real, Categorical, Integer\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "\n",
        "# define bayesian optimization estimator and search spaces\n",
        "# search spaces contained in dictionary, where keys are parameter names and values are skopt.space.Dimension instances\n",
        "\n",
        "\n",
        "\n",
        "# Setting the search space\n",
        "search_spaces = {'my_classifier__learning_rate ': Real(0.01, 1.0, 'uniform'), # Step size shrinkage; Boosting learning rate\n",
        "                 'my_classifier__max_depth': Integer(2, 12), # maximum tree depth for base learners\n",
        "                 'my_classifier__subsample': Real(0.1, 1.0, 'uniform'), # subsample ratio of the training instances\n",
        "                 'my_classifier__colsample_bytree': Real(0.1, 1.0, 'uniform'), # subsample ratio of columns when constructing each tree\n",
        "                 'my_classifier__reg_lambda': Real(1e-5, 100., 'uniform'), # L2 regularization term on weights\n",
        "                 'my_classifier__reg_alpha': Real(1e-5, 100., 'uniform'), #  L1 regularization term on weights\n",
        "                 'my_classifier__n_estimators': Integer(1, 500) # number of boosting rounds\n",
        "   }\n",
        "\n",
        "\n",
        "bayes_search = BayesSearchCV(\n",
        "    full_pipline,\n",
        "    search_spaces=search_spaces,\n",
        "    n_iter=4, # number of parameter settings that are sampled\n",
        "    random_state=0, # use random numbers generated for random_state=0\n",
        "    verbose=1, # set verbosity to 1 for level of details of CV\n",
        "    cv=3, # 3-fold cross validation for each combination of parameters,\n",
        "    scoring='roc_auc'\n",
        ")\n",
        "\n",
        "bayes_search.fit(x, y) # execute bayesian optimization\n",
        "\n",
        "print('bayesXG best score {}'.format(bayes_search.best_score_)) # print best score from bayesian optimization\n",
        "print('bayesXG best params {}'.format(bayes_search.best_params_)) # print parameters associated with best score"
      ],
      "id": "U0ITlsrIIBTq",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikit-learn==0.23.2\n",
            "  Using cached scikit_learn-0.23.2-cp37-cp37m-manylinux1_x86_64.whl (6.8 MB)\n",
            "Collecting joblib>=0.11\n",
            "  Using cached joblib-1.1.0-py2.py3-none-any.whl (306 kB)\n",
            "Collecting scipy>=0.19.1\n",
            "  Using cached scipy-1.7.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (28.5 MB)\n",
            "Collecting threadpoolctl>=2.0.0\n",
            "  Using cached threadpoolctl-3.0.0-py3-none-any.whl (14 kB)\n",
            "Collecting numpy>=1.13.3\n",
            "  Using cached numpy-1.21.3-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
            "Installing collected packages: numpy, threadpoolctl, scipy, joblib, scikit-learn\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.6.0 requires numpy~=1.19.2, but you have numpy 1.21.3 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed joblib-1.1.0 numpy-1.21.3 scikit-learn-0.23.2 scipy-1.7.1 threadpoolctl-3.0.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "joblib",
                  "numpy",
                  "scipy",
                  "sklearn"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.7/dist-packages (0.90)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from xgboost) (1.21.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from xgboost) (1.7.1)\n",
            "Requirement already satisfied: scikit-optimize in /usr/local/lib/python3.7/dist-packages (0.9.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scikit-optimize) (1.21.3)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit-optimize) (1.7.1)\n",
            "Requirement already satisfied: pyaml>=16.9 in /usr/local/lib/python3.7/dist-packages (from scikit-optimize) (21.10.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.7/dist-packages (from scikit-optimize) (0.23.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-optimize) (1.1.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from pyaml>=16.9->scikit-optimize) (3.13)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->scikit-optimize) (3.0.0)\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:   10.3s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:   21.3s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    7.9s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    7.3s finished\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bayesXG best score 0.876074118973488\n",
            "bayesXG best params OrderedDict([('my_classifier__colsample_bytree', 0.5758225678679059), ('my_classifier__learning_rate ', 0.06731886525659495), ('my_classifier__max_depth', 5), ('my_classifier__n_estimators', 97), ('my_classifier__reg_alpha', 24.046322825966453), ('my_classifier__reg_lambda', 34.21270911008482), ('my_classifier__subsample', 0.6778671495585501)])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "jwJ2XYjFKizp",
        "outputId": "3bfe342a-3fdb-45db-acb5-01b2171e7b7e"
      },
      "source": [
        "# create submission\n",
        "submission = pd.DataFrame() # create data frame\n",
        "submission['id'] = data_test['id'] # set 'id' column elements with those from 'id' in data_test\n",
        "submission['match'] = bayes_search.predict_proba(data_test)[:,1] # probability of data belonging to class 1\n",
        "submission.to_csv('submission_trial9.csv', index=False) # save data to csv file without creating index\n",
        "submission # preview data"
      ],
      "id": "jwJ2XYjFKizp",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>match</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>934</td>\n",
              "      <td>0.322627</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>6539</td>\n",
              "      <td>0.581519</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>6757</td>\n",
              "      <td>0.624071</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2275</td>\n",
              "      <td>0.132248</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1052</td>\n",
              "      <td>0.077497</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2464</th>\n",
              "      <td>7982</td>\n",
              "      <td>0.671796</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2465</th>\n",
              "      <td>7299</td>\n",
              "      <td>0.661925</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2466</th>\n",
              "      <td>1818</td>\n",
              "      <td>0.240758</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2467</th>\n",
              "      <td>937</td>\n",
              "      <td>0.100183</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2468</th>\n",
              "      <td>6691</td>\n",
              "      <td>0.091310</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2469 rows √ó 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        id     match\n",
              "0      934  0.322627\n",
              "1     6539  0.581519\n",
              "2     6757  0.624071\n",
              "3     2275  0.132248\n",
              "4     1052  0.077497\n",
              "...    ...       ...\n",
              "2464  7982  0.671796\n",
              "2465  7299  0.661925\n",
              "2466  1818  0.240758\n",
              "2467   937  0.100183\n",
              "2468  6691  0.091310\n",
              "\n",
              "[2469 rows x 2 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afwegONeRTCg"
      },
      "source": [
        "### **Trial10**\n",
        "\n",
        "Score with increased search spaces is similar to previous trial, submission score did not improve.\n",
        "\n",
        "**bayesXG best score 0.876074118973488**\n",
        "\n",
        "**bayesXG best params** : ([('my_classifier__colsample_bytree', 0.5758225678679059), ('my_classifier__learning_rate ', 0.06731886525659495), ('my_classifier__max_depth', 5), ('my_classifier__n_estimators', 97), ('my_classifier__reg_alpha', 24.046322825966453), ('my_classifier__reg_lambda', 34.21270911008482), ('my_classifier__subsample', 0.6778671495585501)])\n",
        "\n",
        "**submission scored 0.87613**\n",
        "\n",
        "I will attempt to use SelectKBest for univariate feature selection for this trial. I predict the model score may increase if there is less noise using feature reduction."
      ],
      "id": "afwegONeRTCg"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 871
        },
        "id": "vTOSyAu0RxRB",
        "outputId": "0412bcd2-1400-4282-d052-4eb4caabeadc"
      },
      "source": [
        "# Trial10\n",
        "\n",
        "# load modules and functions\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import f1_score\n",
        "from pprint import pprint\n",
        "\n",
        "# read in data\n",
        "\n",
        "data = pd.read_csv('train.csv')\n",
        "data_test = pd.read_csv('test.csv')\n",
        "# data.shape # return shape of the array, which is 5909 rows and 192 columns\n",
        "\n",
        "# data.isnull().sum().hist() # count of null values for columns as histogram\n",
        "# data['match'].hist() # count of 'match' column as histogram; binary, 1 is positive match\n",
        "\n",
        "#======================================================\n",
        "# down-grade scikit-learn (latest not greatest :)\n",
        "!pip install -Ivq scikit-learn==0.23.2\n",
        "\n",
        "# if you haven't installed xgboost on your system, uncomment the line below\n",
        "!pip install xgboost\n",
        "# if you haven't installed bayesian-optimization on your system, uncomment the line below\n",
        "!pip install scikit-optimize\n",
        "\n",
        "#======================================================\n",
        "x = data.drop('match', axis=1) # subset data as 'x' without 'match' feature\n",
        "features_numeric = list(x.select_dtypes(include=['float64'])) # subset all features with data type 'float64'\n",
        "features_categorical = list(x.select_dtypes(include=['object'])) # subset all features with data type 'object'\n",
        "y = data['match'] # the dependent feature or response variable 'match'\n",
        "\n",
        "# print(features_categorical) # print categorical features\n",
        "\n",
        "#====================================================\n",
        "\n",
        "# load modules and functions\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "# explicitly require this experimental feature\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "# now you can import normally from sklearn.impute\n",
        "from sklearn.impute import IterativeImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from xgboost.sklearn import XGBClassifier\n",
        "from sklearn.linear_model import BayesianRidge\n",
        "from sklearn.ensemble import ExtraTreesRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
        "\n",
        "np.random.seed(0) # seed the number generator\n",
        "# define steps with name and estimator object for numeric feaeture transformation pipeline\n",
        "transformer_numeric = Pipeline(\n",
        "    steps=[\n",
        "        ('imputer', SimpleImputer(strategy='mean')), ### imputation using IterativeImputer with default 10 imputation rounds and Bayesian Ridge estimator\n",
        "        ('scaler', StandardScaler())] # standardize features by removing mean and scaling to unit variance\n",
        ")\n",
        "# define pipeline steps for categorical feature transformation\n",
        "transformer_categorical = Pipeline(\n",
        "    steps=[\n",
        "        ('imputer', SimpleImputer(strategy='most_frequent')), # replace missing values with most frequent\n",
        "        ('onehot', OneHotEncoder(handle_unknown='ignore')) # columns will be all zeros if unknown categorical feature present during transform\n",
        "    ]\n",
        ")\n",
        "# specify transformers for preprocessing\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', transformer_numeric, features_numeric), # specify numeric transformer application to numeric features subset\n",
        "        ('cat', transformer_categorical, features_categorical) # specify categorical transformer application to categorical subset\n",
        "    ]\n",
        ")\n",
        "# specify full pipeline steps for preprocessing and estimator/classifier\n",
        "scale_pos_value = (4921/988) # value for scale_pos_weight\n",
        "full_pipline = Pipeline(\n",
        "    steps=[\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('selector', SelectKBest(mutual_info_classif, k=5)), # add univariate feature selection; estimate mutual information for a discrete target variable.\n",
        "        ('my_classifier',\n",
        "           XGBClassifier(objective='binary:logistic', scale_pos_weight=scale_pos_value, seed=1), # set scale_pos_weight to sum(negative class)/sum(positive class)\n",
        "          #  RandomForestClassifier(),\n",
        "        )\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "# import classes\n",
        "\n",
        "from skopt import BayesSearchCV\n",
        "from skopt.space import Real, Categorical, Integer\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "\n",
        "# define bayesian optimization estimator and search spaces\n",
        "# search spaces contained in dictionary, where keys are parameter names and values are skopt.space.Dimension instances\n",
        "\n",
        "\n",
        "\n",
        "# Setting the search space\n",
        "search_spaces = {'my_classifier__learning_rate ': Real(0.01, 1.0, 'uniform'), # Step size shrinkage; Boosting learning rate\n",
        "                 'my_classifier__max_depth': Integer(2, 12), # maximum tree depth for base learners\n",
        "                 'my_classifier__subsample': Real(0.1, 1.0, 'uniform'), # subsample ratio of the training instances\n",
        "                 'my_classifier__colsample_bytree': Real(0.1, 1.0, 'uniform'), # subsample ratio of columns when constructing each tree\n",
        "                 'my_classifier__reg_lambda': Real(1e-5, 100., 'uniform'), # L2 regularization term on weights\n",
        "                 'my_classifier__reg_alpha': Real(1e-5, 100., 'uniform'), #  L1 regularization term on weights\n",
        "                 'my_classifier__n_estimators': Integer(1, 500), # number of boosting rounds\n",
        "                 'selector__k': Integer(5, 60)} # number of features to keep\n",
        "\n",
        "\n",
        "bayes_search = BayesSearchCV(\n",
        "    full_pipline,\n",
        "    search_spaces=search_spaces,\n",
        "    n_iter=4, # number of parameter settings that are sampled\n",
        "    random_state=0, # use random numbers generated for random_state=0\n",
        "    verbose=1, # set verbosity to 1 for level of details of CV\n",
        "    cv=3, # 3-fold cross validation for each combination of parameters,\n",
        "    scoring='roc_auc'\n",
        ")\n",
        "\n",
        "bayes_search.fit(x, y) # execute bayesian optimization\n",
        "\n",
        "print('bayesXG best score {}'.format(bayes_search.best_score_)) # print best score from bayesian optimization\n",
        "print('bayesXG best params {}'.format(bayes_search.best_params_)) # print parameters associated with best score"
      ],
      "id": "vTOSyAu0RxRB",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikit-learn==0.23.2\n",
            "  Using cached scikit_learn-0.23.2-cp37-cp37m-manylinux1_x86_64.whl (6.8 MB)\n",
            "Collecting scipy>=0.19.1\n",
            "  Using cached scipy-1.7.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (28.5 MB)\n",
            "Collecting numpy>=1.13.3\n",
            "  Using cached numpy-1.21.3-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
            "Collecting threadpoolctl>=2.0.0\n",
            "  Using cached threadpoolctl-3.0.0-py3-none-any.whl (14 kB)\n",
            "Collecting joblib>=0.11\n",
            "  Using cached joblib-1.1.0-py2.py3-none-any.whl (306 kB)\n",
            "Installing collected packages: numpy, threadpoolctl, scipy, joblib, scikit-learn\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.6.0 requires numpy~=1.19.2, but you have numpy 1.21.3 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed joblib-1.1.0 numpy-1.21.3 scikit-learn-0.23.2 scipy-1.7.1 threadpoolctl-3.0.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "joblib",
                  "numpy",
                  "scipy",
                  "sklearn"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.7/dist-packages (0.90)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from xgboost) (1.21.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from xgboost) (1.7.1)\n",
            "Requirement already satisfied: scikit-optimize in /usr/local/lib/python3.7/dist-packages (0.9.0)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.7/dist-packages (from scikit-optimize) (0.23.2)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scikit-optimize) (1.21.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-optimize) (1.1.0)\n",
            "Requirement already satisfied: pyaml>=16.9 in /usr/local/lib/python3.7/dist-packages (from scikit-optimize) (21.10.1)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit-optimize) (1.7.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from pyaml>=16.9->scikit-optimize) (3.13)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->scikit-optimize) (3.0.0)\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    8.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:   13.6s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    8.8s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    8.8s finished\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bayesXG best score 0.8732600524683966\n",
            "bayesXG best params OrderedDict([('my_classifier__colsample_bytree', 0.33253449577547334), ('my_classifier__learning_rate ', 0.979114621842098), ('my_classifier__max_depth', 11), ('my_classifier__n_estimators', 300), ('my_classifier__reg_alpha', 26.252432978633173), ('my_classifier__reg_lambda', 16.27825791380056), ('my_classifier__subsample', 0.7954522611695781), ('selector__k', 37)])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "oAakwXcxLnU-",
        "outputId": "e9cf2099-c231-4cc8-b74f-ade6a311fdb2"
      },
      "source": [
        "# create submission\n",
        "submission = pd.DataFrame() # create data frame\n",
        "submission['id'] = data_test['id'] # set 'id' column elements with those from 'id' in data_test\n",
        "submission['match'] = bayes_search.predict_proba(data_test)[:,1] # probability of data belonging to class 1\n",
        "submission.to_csv('submission_trial10.csv', index=False) # save data to csv file without creating index\n",
        "submission # preview data"
      ],
      "id": "oAakwXcxLnU-",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>match</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>934</td>\n",
              "      <td>0.116625</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>6539</td>\n",
              "      <td>0.677416</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>6757</td>\n",
              "      <td>0.665763</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2275</td>\n",
              "      <td>0.098645</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1052</td>\n",
              "      <td>0.111033</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2464</th>\n",
              "      <td>7982</td>\n",
              "      <td>0.438978</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2465</th>\n",
              "      <td>7299</td>\n",
              "      <td>0.523689</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2466</th>\n",
              "      <td>1818</td>\n",
              "      <td>0.353202</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2467</th>\n",
              "      <td>937</td>\n",
              "      <td>0.050595</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2468</th>\n",
              "      <td>6691</td>\n",
              "      <td>0.041152</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2469 rows √ó 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        id     match\n",
              "0      934  0.116625\n",
              "1     6539  0.677416\n",
              "2     6757  0.665763\n",
              "3     2275  0.098645\n",
              "4     1052  0.111033\n",
              "...    ...       ...\n",
              "2464  7982  0.438978\n",
              "2465  7299  0.523689\n",
              "2466  1818  0.353202\n",
              "2467   937  0.050595\n",
              "2468  6691  0.041152\n",
              "\n",
              "[2469 rows x 2 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svYSVvo7bnZ5"
      },
      "source": [
        "### **Trial11**\n",
        "This score is similar to the previous trial, submission score did not improve.\n",
        "\n",
        "**bayesXG best score 0.8745876808658064**\n",
        "\n",
        "**bayesXG best params** : ([('my_classifier__colsample_bytree', 0.33253449577547334), ('my_classifier__learning_rate ', 0.979114621842098), ('my_classifier__max_depth', 11), ('my_classifier__n_estimators', 300), ('my_classifier__reg_alpha', 26.252432978633173), ('my_classifier__reg_lambda', 16.27825791380056), ('my_classifier__subsample', 0.7954522611695781), ('selector__k', 40)])\n",
        "\n",
        "**submission scored 0.88569**\n",
        "\n",
        "The next selector I will try is TruncatedSVD. I predict the score will increase if noise is removed."
      ],
      "id": "svYSVvo7bnZ5"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 871
        },
        "id": "YNoVAEbpfKrf",
        "outputId": "ba2eb762-e09b-4cd1-91cd-1328dfac42c7"
      },
      "source": [
        "# Trial11\n",
        "\n",
        "# load modules and functions\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import f1_score\n",
        "from pprint import pprint\n",
        "\n",
        "# read in data\n",
        "\n",
        "data = pd.read_csv('train.csv')\n",
        "data_test = pd.read_csv('test.csv')\n",
        "# data.shape # return shape of the array, which is 5909 rows and 192 columns\n",
        "\n",
        "# data.isnull().sum().hist() # count of null values for columns as histogram\n",
        "# data['match'].hist() # count of 'match' column as histogram; binary, 1 is positive match\n",
        "\n",
        "#======================================================\n",
        "# down-grade scikit-learn (latest not greatest :)\n",
        "!pip install -Ivq scikit-learn==0.23.2\n",
        "\n",
        "# if you haven't installed xgboost on your system, uncomment the line below\n",
        "!pip install xgboost\n",
        "# if you haven't installed bayesian-optimization on your system, uncomment the line below\n",
        "!pip install scikit-optimize\n",
        "\n",
        "#======================================================\n",
        "x = data.drop('match', axis=1) # subset data as 'x' without 'match' feature\n",
        "features_numeric = list(x.select_dtypes(include=['float64'])) # subset all features with data type 'float64'\n",
        "features_categorical = list(x.select_dtypes(include=['object'])) # subset all features with data type 'object'\n",
        "y = data['match'] # the dependent feature or response variable 'match'\n",
        "\n",
        "# print(features_categorical) # print categorical features\n",
        "\n",
        "#====================================================\n",
        "\n",
        "# load modules and functions\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "# explicitly require this experimental feature\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "# now you can import normally from sklearn.impute\n",
        "from sklearn.impute import IterativeImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from xgboost.sklearn import XGBClassifier\n",
        "from sklearn.linear_model import BayesianRidge\n",
        "from sklearn.ensemble import ExtraTreesRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "np.random.seed(0) # seed the number generator\n",
        "# define steps with name and estimator object for numeric feaeture transformation pipeline\n",
        "transformer_numeric = Pipeline(\n",
        "    steps=[\n",
        "        ('imputer', SimpleImputer(strategy='mean')), ### imputation using IterativeImputer with default 10 imputation rounds and Bayesian Ridge estimator\n",
        "        ('scaler', StandardScaler())] # standardize features by removing mean and scaling to unit variance\n",
        ")\n",
        "# define pipeline steps for categorical feature transformation\n",
        "transformer_categorical = Pipeline(\n",
        "    steps=[\n",
        "        ('imputer', SimpleImputer(strategy='most_frequent')), # replace missing values with most frequent\n",
        "        ('onehot', OneHotEncoder(handle_unknown='ignore')) # columns will be all zeros if unknown categorical feature present during transform\n",
        "    ]\n",
        ")\n",
        "# specify transformers for preprocessing\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', transformer_numeric, features_numeric), # specify numeric transformer application to numeric features subset\n",
        "        ('cat', transformer_categorical, features_categorical) # specify categorical transformer application to categorical subset\n",
        "    ]\n",
        ")\n",
        "# specify full pipeline steps for preprocessing and estimator/classifier\n",
        "scale_pos_value = (4921/988) # value for scale_pos_weight\n",
        "full_pipline = Pipeline(\n",
        "    steps=[\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('selector', TruncatedSVD(n_components=5)), # add feature reduction\n",
        "        ('my_classifier',\n",
        "           XGBClassifier(objective='binary:logistic', scale_pos_weight=scale_pos_value, seed=1), # set scale_pos_weight to sum(negative class)/sum(positive class)\n",
        "          #  RandomForestClassifier(),\n",
        "        )\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "# import classes\n",
        "\n",
        "from skopt import BayesSearchCV\n",
        "from skopt.space import Real, Categorical, Integer\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "\n",
        "# define bayesian optimization estimator and search spaces\n",
        "# search spaces contained in dictionary, where keys are parameter names and values are skopt.space.Dimension instances\n",
        "\n",
        "\n",
        "\n",
        "# Setting the search space\n",
        "search_spaces = {'my_classifier__learning_rate ': Real(0.01, 1.0, 'uniform'), # Step size shrinkage; Boosting learning rate\n",
        "                 'my_classifier__max_depth': Integer(2, 12), # maximum tree depth for base learners\n",
        "                 'my_classifier__subsample': Real(0.1, 1.0, 'uniform'), # subsample ratio of the training instances\n",
        "                 'my_classifier__colsample_bytree': Real(0.1, 1.0, 'uniform'), # subsample ratio of columns when constructing each tree\n",
        "                 'my_classifier__reg_lambda': Real(1e-5, 100., 'uniform'), # L2 regularization term on weights\n",
        "                 'my_classifier__reg_alpha': Real(1e-5, 100., 'uniform'), #  L1 regularization term on weights\n",
        "                 'my_classifier__n_estimators': Integer(1, 500), # number of boosting rounds\n",
        "                 'selector__n_components': Integer(10, 100)} # number of features to keep\n",
        "\n",
        "\n",
        "bayes_search = BayesSearchCV(\n",
        "    full_pipline,\n",
        "    search_spaces=search_spaces,\n",
        "    n_iter=4, # number of parameter settings that are sampled\n",
        "    random_state=0, # use random numbers generated for random_state=0\n",
        "    verbose=1, # set verbosity to 1 for level of details of CV\n",
        "    cv=3, # 3-fold cross validation for each combination of parameters,\n",
        "    scoring='roc_auc'\n",
        ")\n",
        "\n",
        "bayes_search.fit(x, y) # execute bayesian optimization\n",
        "\n",
        "print('bayesXG best score {}'.format(bayes_search.best_score_)) # print best score from bayesian optimization\n",
        "print('bayesXG best params {}'.format(bayes_search.best_params_)) # print parameters associated with best score"
      ],
      "id": "YNoVAEbpfKrf",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikit-learn==0.23.2\n",
            "  Using cached scikit_learn-0.23.2-cp37-cp37m-manylinux1_x86_64.whl (6.8 MB)\n",
            "Collecting numpy>=1.13.3\n",
            "  Using cached numpy-1.21.3-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
            "Collecting joblib>=0.11\n",
            "  Using cached joblib-1.1.0-py2.py3-none-any.whl (306 kB)\n",
            "Collecting scipy>=0.19.1\n",
            "  Using cached scipy-1.7.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (28.5 MB)\n",
            "Collecting threadpoolctl>=2.0.0\n",
            "  Using cached threadpoolctl-3.0.0-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: numpy, threadpoolctl, scipy, joblib, scikit-learn\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.6.0 requires numpy~=1.19.2, but you have numpy 1.21.3 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed joblib-1.1.0 numpy-1.21.3 scikit-learn-0.23.2 scipy-1.7.1 threadpoolctl-3.0.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "joblib",
                  "numpy",
                  "scipy",
                  "sklearn"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.7/dist-packages (0.90)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from xgboost) (1.21.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from xgboost) (1.7.1)\n",
            "Requirement already satisfied: scikit-optimize in /usr/local/lib/python3.7/dist-packages (0.9.0)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.7/dist-packages (from scikit-optimize) (0.23.2)\n",
            "Requirement already satisfied: pyaml>=16.9 in /usr/local/lib/python3.7/dist-packages (from scikit-optimize) (21.10.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scikit-optimize) (1.21.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-optimize) (1.1.0)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit-optimize) (1.7.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from pyaml>=16.9->scikit-optimize) (3.13)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->scikit-optimize) (3.0.0)\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    2.4s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:   15.5s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    5.3s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    4.8s finished\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bayesXG best score 0.8339274234721734\n",
            "bayesXG best params OrderedDict([('my_classifier__colsample_bytree', 0.33253449577547334), ('my_classifier__learning_rate ', 0.979114621842098), ('my_classifier__max_depth', 11), ('my_classifier__n_estimators', 300), ('my_classifier__reg_alpha', 26.252432978633173), ('my_classifier__reg_lambda', 16.27825791380056), ('my_classifier__subsample', 0.7954522611695781), ('selector__n_components', 62)])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "wJZDO77gMGd5",
        "outputId": "3751dbd6-5dce-49e0-e844-5c29fac9c6e8"
      },
      "source": [
        "# create submission\n",
        "submission = pd.DataFrame() # create data frame\n",
        "submission['id'] = data_test['id'] # set 'id' column elements with those from 'id' in data_test\n",
        "submission['match'] = bayes_search.predict_proba(data_test)[:,1] # probability of data belonging to class 1\n",
        "submission.to_csv('submission_trial11.csv', index=False) # save data to csv file without creating index\n",
        "submission # preview data"
      ],
      "id": "wJZDO77gMGd5",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>match</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>934</td>\n",
              "      <td>0.188443</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>6539</td>\n",
              "      <td>0.521988</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>6757</td>\n",
              "      <td>0.168083</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2275</td>\n",
              "      <td>0.062937</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1052</td>\n",
              "      <td>0.026610</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2464</th>\n",
              "      <td>7982</td>\n",
              "      <td>0.591594</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2465</th>\n",
              "      <td>7299</td>\n",
              "      <td>0.729082</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2466</th>\n",
              "      <td>1818</td>\n",
              "      <td>0.304438</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2467</th>\n",
              "      <td>937</td>\n",
              "      <td>0.015972</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2468</th>\n",
              "      <td>6691</td>\n",
              "      <td>0.032666</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2469 rows √ó 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        id     match\n",
              "0      934  0.188443\n",
              "1     6539  0.521988\n",
              "2     6757  0.168083\n",
              "3     2275  0.062937\n",
              "4     1052  0.026610\n",
              "...    ...       ...\n",
              "2464  7982  0.591594\n",
              "2465  7299  0.729082\n",
              "2466  1818  0.304438\n",
              "2467   937  0.015972\n",
              "2468  6691  0.032666\n",
              "\n",
              "[2469 rows x 2 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBpb5Zusp4Cn"
      },
      "source": [
        "### **Trial12**\n",
        "\n",
        "The score has decreased, possibly because useful data was removed from TruncatedSVD. Since there were worse fits for training and test scores, I assume it is underfitting. The removed features could have hindered the model from sufficiently capturing the true trend.\n",
        "\n",
        "**bayesXG best score 0.8339274234721734**\n",
        "\n",
        "**bayesXG best params** : ([('my_classifier__colsample_bytree', 0.33253449577547334), ('my_classifier__learning_rate ', 0.979114621842098), ('my_classifier__max_depth', 11), ('my_classifier__n_estimators', 300), ('my_classifier__reg_alpha', 26.252432978633173), ('my_classifier__reg_lambda', 16.27825791380056), ('my_classifier__subsample', 0.7954522611695781), ('selector__n_components', 62)])\n",
        "\n",
        "**submission scored 0.84741**\n",
        "\n",
        "For trial12 I will attempt to further optimize the model by setting the following parameters:\n",
        "\n",
        "n_iter=20\n",
        "\n",
        "cv=5\n",
        "\n",
        "'my_classifier__learning_rate ': Real(0.01, 0.2, 'uniform')\n",
        "\n",
        "The learning rate may be too high with 1.0 as the upper limit. I lower it to reduce the possibility of converging to quickly, which yields a suboptimal solution. I also increase n_iter to sample more parameters and increase cv to 5 to use more training data in each iteration.\n",
        "\n"
      ],
      "id": "CBpb5Zusp4Cn"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-rInyTw3NDtb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "702e0ad2-98bc-45bb-bec8-982eac79e580"
      },
      "source": [
        "# Trial12\n",
        "\n",
        "# load modules and functions\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import f1_score\n",
        "from pprint import pprint\n",
        "\n",
        "# read in data\n",
        "\n",
        "data = pd.read_csv('train.csv')\n",
        "data_test = pd.read_csv('test.csv')\n",
        "# data.shape # return shape of the array, which is 5909 rows and 192 columns\n",
        "\n",
        "# data.isnull().sum().hist() # count of null values for columns as histogram\n",
        "# data['match'].hist() # count of 'match' column as histogram; binary, 1 is positive match\n",
        "\n",
        "#======================================================\n",
        "# down-grade scikit-learn (latest not greatest :)\n",
        "!pip install -Ivq scikit-learn==0.23.2\n",
        "\n",
        "# if you haven't installed xgboost on your system, uncomment the line below\n",
        "!pip install xgboost\n",
        "# if you haven't installed bayesian-optimization on your system, uncomment the line below\n",
        "!pip install scikit-optimize\n",
        "\n",
        "#======================================================\n",
        "x = data.drop('match', axis=1) # subset data as 'x' without 'match' feature\n",
        "features_numeric = list(x.select_dtypes(include=['float64'])) # subset all features with data type 'float64'\n",
        "features_categorical = list(x.select_dtypes(include=['object'])) # subset all features with data type 'object'\n",
        "y = data['match'] # the dependent feature or response variable 'match'\n",
        "\n",
        "# print(features_categorical) # print categorical features\n",
        "\n",
        "#====================================================\n",
        "\n",
        "# load modules and functions\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "# explicitly require this experimental feature\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "# now you can import normally from sklearn.impute\n",
        "from sklearn.impute import IterativeImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from xgboost.sklearn import XGBClassifier\n",
        "from sklearn.linear_model import BayesianRidge\n",
        "from sklearn.ensemble import ExtraTreesRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
        "\n",
        "np.random.seed(0) # seed the number generator\n",
        "# define steps with name and estimator object for numeric feaeture transformation pipeline\n",
        "transformer_numeric = Pipeline(\n",
        "    steps=[\n",
        "        ('imputer', SimpleImputer(strategy='mean')), ### imputation using IterativeImputer with default 10 imputation rounds and Bayesian Ridge estimator\n",
        "        ('scaler', StandardScaler())] # standardize features by removing mean and scaling to unit variance\n",
        ")\n",
        "# define pipeline steps for categorical feature transformation\n",
        "transformer_categorical = Pipeline(\n",
        "    steps=[\n",
        "        ('imputer', SimpleImputer(strategy='most_frequent')), # replace missing values with most frequent\n",
        "        ('onehot', OneHotEncoder(handle_unknown='ignore')) # columns will be all zeros if unknown categorical feature present during transform\n",
        "    ]\n",
        ")\n",
        "# specify transformers for preprocessing\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', transformer_numeric, features_numeric), # specify numeric transformer application to numeric features subset\n",
        "        ('cat', transformer_categorical, features_categorical) # specify categorical transformer application to categorical subset\n",
        "    ]\n",
        ")\n",
        "# specify full pipeline steps for preprocessing and estimator/classifier\n",
        "scale_pos_value = (4921/988) # value for scale_pos_weight\n",
        "full_pipline = Pipeline(\n",
        "    steps=[\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('selector', SelectKBest(mutual_info_classif, k=5)), # add univariate feature selection; estimate mutual information for a discrete target variable.\n",
        "        ('my_classifier',\n",
        "           XGBClassifier(objective='binary:logistic', scale_pos_weight=scale_pos_value, seed=1), # set scale_pos_weight to sum(negative class)/sum(positive class)\n",
        "          #  RandomForestClassifier(),\n",
        "        )\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "# import classes\n",
        "\n",
        "from skopt import BayesSearchCV\n",
        "from skopt.space import Real, Categorical, Integer\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "\n",
        "# define bayesian optimization estimator and search spaces\n",
        "# search spaces contained in dictionary, where keys are parameter names and values are skopt.space.Dimension instances\n",
        "\n",
        "\n",
        "\n",
        "# Setting the search space\n",
        "search_spaces = {'my_classifier__learning_rate ': Real(0.01, 0.2, 'uniform'), # Step size shrinkage; Boosting learning rate\n",
        "                 'my_classifier__max_depth': Integer(2, 12), # maximum tree depth for base learners\n",
        "                 'my_classifier__subsample': Real(0.1, 1.0, 'uniform'), # subsample ratio of the training instances\n",
        "                 'my_classifier__colsample_bytree': Real(0.1, 1.0, 'uniform'), # subsample ratio of columns when constructing each tree\n",
        "                 'my_classifier__reg_lambda': Real(1e-5, 100., 'uniform'), # L2 regularization term on weights\n",
        "                 'my_classifier__reg_alpha': Real(1e-5, 100., 'uniform'), #  L1 regularization term on weights\n",
        "                 'my_classifier__n_estimators': Integer(1, 500), # number of boosting rounds\n",
        "                 'selector__k': Integer(5, 60)}\n",
        "\n",
        "\n",
        "bayes_search = BayesSearchCV(\n",
        "    full_pipline,\n",
        "    search_spaces=search_spaces,\n",
        "    n_iter=20, # number of parameter settings that are sampled\n",
        "    random_state=0, # use random numbers generated for random_state=0\n",
        "    verbose=1, # set verbosity to 1 for level of details of CV\n",
        "    cv=5, # 5-fold cross validation for each combination of parameters,\n",
        "    scoring='roc_auc'\n",
        ")\n",
        "\n",
        "bayes_search.fit(x, y) # execute bayesian optimization\n",
        "\n",
        "print('bayesXG best score {}'.format(bayes_search.best_score_)) # print best score from bayesian optimization\n",
        "print('bayesXG best params {}'.format(bayes_search.best_params_)) # print parameters associated with best score"
      ],
      "id": "-rInyTw3NDtb",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikit-learn==0.23.2\n",
            "  Using cached scikit_learn-0.23.2-cp37-cp37m-manylinux1_x86_64.whl (6.8 MB)\n",
            "Collecting joblib>=0.11\n",
            "  Using cached joblib-1.1.0-py2.py3-none-any.whl (306 kB)\n",
            "Collecting scipy>=0.19.1\n",
            "  Using cached scipy-1.7.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (28.5 MB)\n",
            "Collecting threadpoolctl>=2.0.0\n",
            "  Using cached threadpoolctl-3.0.0-py3-none-any.whl (14 kB)\n",
            "Collecting numpy>=1.13.3\n",
            "  Using cached numpy-1.21.3-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
            "Installing collected packages: numpy, threadpoolctl, scipy, joblib, scikit-learn\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.6.0 requires numpy~=1.19.2, but you have numpy 1.21.3 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed joblib-1.1.0 numpy-1.21.3 scikit-learn-0.23.2 scipy-1.7.1 threadpoolctl-3.0.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "joblib",
                  "numpy",
                  "scipy",
                  "sklearn"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.7/dist-packages (0.90)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from xgboost) (1.7.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from xgboost) (1.21.3)\n",
            "Requirement already satisfied: scikit-optimize in /usr/local/lib/python3.7/dist-packages (0.9.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scikit-optimize) (1.21.3)\n",
            "Requirement already satisfied: pyaml>=16.9 in /usr/local/lib/python3.7/dist-packages (from scikit-optimize) (21.10.1)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit-optimize) (1.7.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-optimize) (1.1.0)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.7/dist-packages (from scikit-optimize) (0.23.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from pyaml>=16.9->scikit-optimize) (3.13)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->scikit-optimize) (3.0.0)\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:   14.4s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:   26.2s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:   15.9s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:   15.9s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:   13.6s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:   32.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:   20.4s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:   13.3s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:   15.3s finished\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:   24.9s finished\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:   18.9s finished\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:   17.4s finished\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:   47.9s finished\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:   19.4s finished\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:   13.0s finished\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:   21.8s finished\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  1.3min finished\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:   23.7s finished\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:   16.9s finished\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:   23.8s finished\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bayesXG best score 0.8830114358163705\n",
            "bayesXG best params OrderedDict([('my_classifier__colsample_bytree', 0.7605435214401954), ('my_classifier__learning_rate ', 0.2), ('my_classifier__max_depth', 6), ('my_classifier__n_estimators', 418), ('my_classifier__reg_alpha', 1e-05), ('my_classifier__reg_lambda', 64.78163812835714), ('my_classifier__subsample', 0.6985647441016547), ('selector__k', 60)])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "AfqksowAejoE",
        "outputId": "9e7b05ac-6523-46aa-fc22-89b0ab921e88"
      },
      "source": [
        "# create submission\n",
        "submission = pd.DataFrame() # create data frame\n",
        "submission['id'] = data_test['id'] # set 'id' column elements with those from 'id' in data_test\n",
        "submission['match'] = bayes_search.predict_proba(data_test)[:,1] # probability of data belonging to class 1\n",
        "submission.to_csv('submission_trial12.csv', index=False) # save data to csv file without creating index\n",
        "submission # preview data"
      ],
      "id": "AfqksowAejoE",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>match</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>934</td>\n",
              "      <td>0.080949</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>6539</td>\n",
              "      <td>0.619202</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>6757</td>\n",
              "      <td>0.612094</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2275</td>\n",
              "      <td>0.038372</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1052</td>\n",
              "      <td>0.048906</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2464</th>\n",
              "      <td>7982</td>\n",
              "      <td>0.252640</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2465</th>\n",
              "      <td>7299</td>\n",
              "      <td>0.801036</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2466</th>\n",
              "      <td>1818</td>\n",
              "      <td>0.233977</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2467</th>\n",
              "      <td>937</td>\n",
              "      <td>0.012017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2468</th>\n",
              "      <td>6691</td>\n",
              "      <td>0.015402</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2469 rows √ó 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        id     match\n",
              "0      934  0.080949\n",
              "1     6539  0.619202\n",
              "2     6757  0.612094\n",
              "3     2275  0.038372\n",
              "4     1052  0.048906\n",
              "...    ...       ...\n",
              "2464  7982  0.252640\n",
              "2465  7299  0.801036\n",
              "2466  1818  0.233977\n",
              "2467   937  0.012017\n",
              "2468  6691  0.015402\n",
              "\n",
              "[2469 rows x 2 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27HwB45em6AR"
      },
      "source": [
        "### **End of Trials**\n",
        "\n",
        "Trial12 results:\n",
        "\n",
        "**bayesXG best score 0.8830114358163705**\n",
        "\n",
        "**bayesXG best params:** ([('my_classifier__colsample_bytree', 0.7605435214401954), ('my_classifier__learning_rate ', 0.2), ('my_classifier__max_depth', 6), ('my_classifier__n_estimators', 418), ('my_classifier__reg_alpha', 1e-05), ('my_classifier__reg_lambda', 64.78163812835714), ('my_classifier__subsample', 0.6985647441016547), ('selector__k', 60)])\n",
        "\n",
        "submission scored 0.88933\n",
        "\n",
        "This was the best scoring trial. I believe the adjustments made helped the model to reach a more optimal solution, although it was only a slight improvement. Overall I did not observe any overfitting, as the training and test scores were very similar in each trial. There may have been some underfitting due to dimensionality reduction for trial11 while using TruncatedSVD."
      ],
      "id": "27HwB45em6AR"
    }
  ]
}